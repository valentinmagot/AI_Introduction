{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsCQnroFWbpA"
   },
   "source": [
    "# Notebook 10 - Apprentissage par renforcement / Taxi autonome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jS9f1M7hWblx"
   },
   "source": [
    "CSI4106 Intelligence Artificielle   \n",
    "Automne 2020  \n",
    "Preparé par Julian Templeton, Caroline Barrière et Joel Muteba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWknU1E2Wbi8"
   },
   "source": [
    "***INTRODUCTION***:  \n",
    "Dans ce notebook, nous explorerons l'utilisation de l'apprentissage par renforcement pour aider un agent à résoudre une tâche spécifique dans un environnement fourni par la [bibliothèque OpenAI's Gym](https://gym.openai.com/). Cette bibliothèque fournit un certain nombre d'environnements, nous pouvons entraîner notre modèle IA à maîtriser un ou plusieurs de ces environnements. Dans ce notebook, nous explorerons un scénario dans lequel un taxi situé dans une grille doit être contrôlé par un agent pour prendre un passager situé dans l'une de quatre positions et déposer le passager dans l'une de trois autres positions.\n",
    "\n",
    "Pour vous familiariser avec le problème de la cabine autonome abordé dans ce notebook, veuillez vous rendre sur le site https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/ et lire la section 1 (récompenses), section 2 (espace d'états) qui vous fera comprendre pourquoi il y a 500 états possibles, section 3 (espace d'action) qui décrit les actions possibles.\n",
    "\n",
    "Tout au long du notebook, nous travaillerons avec une approche de base et une approche basée sur le Q-Learning. Cela fournira un aperçu de la façon dont Q-Learning peut être appliqué aux problèmes et comment un agent peut utiliser l'apprentissage par renforcement pour résoudre des problèmes dans un environnement.\n",
    "\n",
    "**Lors de la soumission de ce notebook, assurez-vous de NE PAS réinitialiser les sorties de l'exécution du code (et n'oubliez pas d'enregistrer le notebook avec ctrl + s).**\n",
    "\n",
    "**Afin de faciliter l'installation, vous exécuterez à nouveau ce notebook dans Google Colab, PAS sur votre ordinateur local.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iCMGO9VWbgM"
   },
   "source": [
    "***Devoir***:  \n",
    "Parcourez le notebook en exécutant chaque cellule, une à la fois.\n",
    "Recherchez **(TO DO)** pour les tâches que vous devez effectuer. Ne modifiez pas le code en dehors des questions auxquelles vous êtes invité à répondre à moins que cela ne vous soit spécifiquement demandé. Une fois que vous avez terminé, signez le notebook (à la fin du notebook) et soumettez-le.\n",
    "\n",
    "*Le notebook sera noté sur 30.\n",
    "Chaque **(TO DO)** a un certain nombre de points qui lui sont associés.*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIhGNTB-Wbbs"
   },
   "source": [
    "**1.0 - Mise en place du jeu de taxi**   \n",
    "\n",
    "Pour commencer le notebook, nous devrons configurer et explorer l'environnement avec lequel notre agent travaillera. OpenAI's Gym propose de nombreuses expériences différentes à utiliser. Celles-ci vont des actes d'équilibrage aux voitures autonomes en passant par le jeu simple d'Atari. Malheureusement, toutes les options disponibles ne peuvent pas être facilement utilisées. Beaucoup peuvent prendre des heures d'entraînement pour commencer à voir des résultats passionnants. Chacune de ces expériences utilise des agents qui peuvent être entraînés par apprentissage par renforcement pour maîtriser comment exécuter la tâche spécifiée. Les méthodes utilisées peuvent aller de la simple utilisation du Q-Learning à l'utilisation plus complexe d'un ou plusieurs modèles de Deep Learning qui fonctionnent en conjonction avec des techniques d'apprentissage par renforcement.\n",
    "\n",
    "Une expérience simple, mais intéressante, implique un taxi contrôlé par l'IA qui doit prendre et déposer un passager. C'est le problème que nous allons explorer tout au long du notebook. Le code utilisé dans tout le notebook provient de [cet exemple](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/) et a été modifié conséquemment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEQK-Xep0MrF"
   },
   "source": [
    "Pour commencer, nous installerons certains des packages dont nous aurons besoin pour exécuter le programme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BPYIG4d6Ztzk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in c:\\users\\valentin\\anaconda3\\lib\\site-packages (3.18.4.post1)\n",
      "Requirement already satisfied: gym[atari] in c:\\users\\valentin\\anaconda3\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\valentin\\anaconda3\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\valentin\\anaconda3\\lib\\site-packages (from gym[atari]) (1.3.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\valentin\\anaconda3\\lib\\site-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\valentin\\anaconda3\\lib\\site-packages (from gym[atari]) (1.18.1)\n",
      "Requirement already satisfied: opencv-python; extra == \"atari\" in c:\\users\\valentin\\anaconda3\\lib\\site-packages (from gym[atari]) (4.4.0.46)\n",
      "Requirement already satisfied: Pillow; extra == \"atari\" in c:\\users\\valentin\\anaconda3\\lib\\site-packages (from gym[atari]) (7.0.0)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in c:\\users\\valentin\\anaconda3\\lib\\site-packages (from gym[atari]) (0.2.6)\n",
      "Requirement already satisfied: future in c:\\users\\valentin\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.18.2)\n",
      "Requirement already satisfied: six in c:\\users\\valentin\\anaconda3\\lib\\site-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary libraries\n",
    "!pip install cmake \"gym[atari]\" scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "O0vOVuqHA8SM"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import random\n",
    "from random import choice\n",
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3nhPCUHb2cO"
   },
   "source": [
    "Avec toutes les bibliothèques installées, nous allons maintenant utiliser le programme Taxi fourni par Gym. Ci-dessous, nous allons importer Gym, charger le programme comme environnement actif et montrer une image représentant l'état actuel du programme.\n",
    "\n",
    "D'après l'image ci-dessous, il existe quatre emplacements clés différents dans l'environnement, représentés par *R*, *G*, *B* et *Y*. La lettre qui est en gras en bleu représente l'endroit où le passager actuel doit être récupéré et la lettre en gras en violet représente l'endroit où le passager souhaite déposer. Le bloc jaune représente la cellule dans laquelle se trouve actuellement le taxi. Par conséquent, le taxi doit d'abord récupérer le passager et le déposer au lieu de dépose. Lorsqu'un passager est dans le taxi, il devient vert jusqu'à ce que le passager soit déposé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "f2iMY2MbaFSk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the environment\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "# Render the current state of the program\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oq2BvrwdPh-"
   },
   "source": [
    "Ensuite, nous allons réinitialiser l'état de l'environnement et montrer à nouveau l'état actuel. Nous imprimons également le nombre total d'actions disponibles pour notre agent (défini comme *l'espace d'action*) et *l'espace d'état* qui représente l'état du programme (où se trouve la cabine, le passager, le lieu de prise en charge et le lieu où déposer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JlLevJ7KaHYc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGp8quCqeGjU"
   },
   "source": [
    "Intuitivement, nous voulons que notre agent apprenne quelle action entreprendre dans un état spécifique. Plus précisément, quelles mesures devraient être prises en fonction de l'emplacement du taxi par rapport à l'emplacement du passager et au lieu où déposer. Les six actions possibles que le taxi peut effectuer à un étape donnée sont:\n",
    "\n",
    "Action = 0: Aller vers le Sud\n",
    "\n",
    "Action = 1: Aller vers le Nord\n",
    "\n",
    "Action = 2: Aller vers l'Est\n",
    "\n",
    "Action = 3: Aller vers l'Ouest\n",
    "\n",
    "Action = 4: Ramasser/Prendre le passager\n",
    "\n",
    "Action = 5: Déposer le passager\n",
    "\n",
    "Vous trouverez ci-dessous un exemple de définition de l'état avec un codage spécifique et le rendu de cet état."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x7QM71s0aded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The encoding below represents: (taxi row, taxi column, passenger index, destination index)\n",
    "state = env.encode(3, 1, 2, 0) \n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCSHJMfb2Hll"
   },
   "source": [
    "L'exemple suivant montre comment définir un état avec le passager dans le taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xPshGWDomKfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 36\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m:\u001b[42m_\u001b[0m| : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### The encoding below represents: (taxi row, taxi column, passenger index, destination index)\n",
    "state = env.encode(0, 1, 4, 0) \n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3b__QWRTRB3"
   },
   "source": [
    "#### **(TO DO) Q1**\n",
    "Maintenant que nous avons vu comment définir un état via un encodage, vous devrez définir l'état pour qu'il corresponde aux descriptions ci-dessous et les montrer.\n",
    "\n",
    "a) Réglez le passager à la position G, le passager souhaitant être déposé à la position R et le taxi positionné à un point aléatoire sur la grille (la position sélectionnée du taxi doit être choisie au hasard). Après avoir défini la position, montrez le rendu (avec render) de l'état.\n",
    "\n",
    "b) Réglez le passager pour qu'il soit dans le taxi (à n'importe quelle position sans lettre dessus) et réglez le point de débarquement des passagers sur la position B. Après avoir défini la position, restituez (avec render) l'état."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YKbEIP2WAkb"
   },
   "source": [
    "**(TO DO) Q1 (a) - 2 points**    \n",
    "a) Réglez le passager à la position G, le passager souhaitant être déposé à la position R et le taxi positionné à un point aléatoire sur la grille (la position sélectionnée du taxi doit être choisie au hasard). Après avoir défini la position, montrez le rendu (avec render) de l'état."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "yT05ErEOTRPl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 64\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO (remember to use random coordinates within the grid for the taxi)...\n",
    "randRow = random.randint(0, 4)\n",
    "randCol = random.randint(0, 4)\n",
    "state = env.encode(randRow, randCol, 1, 0)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pl8bVtwWIdE"
   },
   "source": [
    "**(TO DO) Q1 (b) - 2 points**\n",
    "\n",
    "b) Réglez le passager pour qu'il soit dans le taxi (à n'importe quelle position sans lettre dessus) et réglez le point de débarquement des passagers sur la position B. Après avoir défini la position, restituez (avec render) l'état."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "6roy4E4NWIv0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 219\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO ...\n",
    "randRow = random.randint(0, 4)\n",
    "randCol = random.randint(0, 4)\n",
    "if randRow ==0 :\n",
    "  randCol = choice([i for i in range(0,4) if i not in [0,4]])\n",
    "  state = env.encode(randRow, randCol, 4,3)\n",
    "elif randRow ==4:\n",
    "  randCol = choice([i for i in range(0,4) if i not in [0,3]])\n",
    "  state = env.encode(randRow, randCol, 4,3)\n",
    "else :\n",
    "  state = env.encode(randRow, randCol, 4,3)\n",
    "\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTyOS_Hbi3Rf"
   },
   "source": [
    "Pour chaque action que le taxi peut entreprendre, nous avons une liste représentant les informations clés par rapport à ce qui se passera lorsqu'une action est effectuée. Après avoir effectué une action, l'agent recevra une récompense ou une pénalité. Cette récompense ou pénalité indiquera à l'agent à quel point sa décision d'effectuer l'action spécifiée était bonne ou mauvaise.\n",
    "\n",
    "Ci-dessous, nous affichons un dictionnaire qui contient toutes les actions possibles ainsi que les informations suivantes dans les tuples correspondants:\n",
    "\n",
    "(\n",
    "\n",
    "   La probabilité de prendre cette action,\n",
    "  \n",
    "   L'état résultant après avoir effectué cette action,\n",
    "  \n",
    "   La récompense pour avoir pris cette action,\n",
    "  \n",
    "   Si le programme se terminera ou non lors de l'exécution de l'action\n",
    "\n",
    ")\n",
    "\n",
    "Exemple de tuple: (1.0, 328, -1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "a6Ga3Y6yagfE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_57Z2adUzAJH"
   },
   "source": [
    "Bien que non affiché par le code ci-dessus, si le taxi contient le passager et est au-dessus du point de débarquement, la récompense pour l'action de déposer le passager est de 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iiy-KNusvBDJ"
   },
   "source": [
    "**2.0 - Approche de base du jeu du taxi**   \n",
    "\n",
    "Pour commencer, nous allons effectuer la simulation du scénario de taxi avec une approche de base qui n'utilise pas Q-Learning. Cette approche fonctionnera simplement en sélectionnant une action aléatoire disponible à chaque pas de temps, quel que soit l'état actuel. Nous préparerons également une méthode de lecture de toutes les images d'un épisode pour voir comment l'agent contrôle le taxi dans le scénario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "dKnHAA5SaitM"
   },
   "outputs": [],
   "source": [
    "def run_single_simulation_baseline(env, state, disable_prints=False):\n",
    "    '''\n",
    "    Given the environment and a specific state, randomly select an action for the taxi\n",
    "    to perform until the goal is completed.\n",
    "    '''\n",
    "    if not disable_prints:\n",
    "        print(\"Testing for simulation: {}\".format(state))\n",
    "    # Set the state of the environment\n",
    "    env.s = state\n",
    "    # Used to hold all information for a single time step (including the image data)\n",
    "    frames = []\n",
    "    # Used to determine when the simulation has been completed\n",
    "    done = False\n",
    "    # Determines the number of times steps that the application has been run for\n",
    "    time_steps = 0\n",
    "    # The total values used to determine how many times the agent mistakenly\n",
    "    # picks up no one or attempts to dropoff no passenger or attempts to\n",
    "    # dropoff a passenger in the wrong position.\n",
    "    penalties, reward = 0, 0\n",
    "    # Run until the passenger has been picked up and dropped off in the target location\n",
    "    while not done:\n",
    "        # Perform a random action from the set of available actions in the environment\n",
    "        action = env.action_space.sample()\n",
    "        # From performing the action, retrieve the new state, the reward from taking the action,\n",
    "        # whether the simulation is complete, and other information from performing the action.\n",
    "        state, reward, done, info = env.step(action)\n",
    "        # If an incorrect dropoff or pickup is performed, increment the penalty count\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        # Put each rendered frame into dict to use for animating the process and\n",
    "        # tracking the details over the run\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "        # Increment the time step count\n",
    "        time_steps += 1\n",
    "    # State the total number of steps taken and the total penalties that have occured.\n",
    "    if not disable_prints:\n",
    "        print(\"Timesteps taken: {}\".format(time_steps))\n",
    "        print(\"Penalties incurred: {}\".format(penalties))\n",
    "    # Return the frame data, the total penalties, and the total time steps\n",
    "    return frames, penalties, time_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk5lkc3Q5XLv"
   },
   "source": [
    "Une fois l'approche de base définie, nous allons exécuter un test avec cette approche pour voir combien de temps il faut à un agent utilisant cette approche pour trouver une solution pour la simulation 328 et combien de pénalités majeures l'agent reçoit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "zTLKSCRq4bEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for simulation: 328\n",
      "Timesteps taken: 173\n",
      "Penalties incurred: 69\n"
     ]
    }
   ],
   "source": [
    "state = 328\n",
    "# Run a test and collect all frames from the run\n",
    "frames, _, _ = run_single_simulation_baseline(env, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayN1up479Fl1"
   },
   "source": [
    "Après avoir effectué une simulation et récupéré les résultats, nous pouvons utiliser les images obtenues à partir de la simulation et les transmettre à la fonction *print_frames* ci-dessous pour afficher une animation contenant toutes les images ainsi que les informations qui provenaient de chaque étape et temps auquel l'image correspond .\n",
    "\n",
    "Pour le premier épisode que vous visualisez, il est recommandé d'exécuter l'ensemble du processus à une vitesse plus lente (telle que 0,3 ou 0,5 dans l'appel de veille). Cependant, vous êtes libre d'augmenter la vitesse du processus en réduisant le nombre dans l'appel de la fonction de veille dans la fonction *print_frames* ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Bt-sMubValJt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 1958\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    '''\n",
    "    For each frame, show the frame and display the timestep it occurred at,\n",
    "    the number of the active state, the action selected, adn the corresponding reward.\n",
    "    '''\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        # Can adjust speed here\n",
    "        sleep(.3)\n",
    "# Print the frames from the episode\n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRCoIPbi7OCv"
   },
   "source": [
    "**(TO DO) Q2**\n",
    "\n",
    "a) En utilisant l'état défini à partir de Q1 (a), récupérer les frames/images correspondantes obtenues en utilisant l'approche de base ci-dessus. Ensuite, affichez ces cadres(frames ou images).\n",
    "\n",
    "b) En utilisant l'état défini à partir de Q1 (b), récupérer les frames/images correspondantes obtenues en utilisant l'approche de base ci-dessus. Ensuite, affichez ces cadres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-N9fP_Ydvot"
   },
   "source": [
    "**(TO DO) Q2 (a) - 2 points**   \n",
    "a) En utilisant l'état défini à partir de Q1 (a), récupérer les frames/images correspondantes obtenues en utilisant l'approche de base ci-dessus. Ensuite, affichez ces cadres(frames ou images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "gDCSqmbM-USp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for simulation: 64\n",
      "Timesteps taken: 999\n",
      "Penalties incurred: 307\n"
     ]
    }
   ],
   "source": [
    "# TODO: Retrieve the corresponding frames from running the simulation starting from the state found in Q1 (a). Then show those frames.\n",
    "state = 64\n",
    "frames, _, _ = run_single_simulation_baseline(env, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3Dufixud28j"
   },
   "source": [
    "**(TO DO) Q2 (b) - 2 points**\n",
    "\n",
    "a) En utilisant l'état défini à partir de Q1 (b), récupérer les frames/images correspondantes obtenues en utilisant l'approche de base ci-dessus. Ensuite, affichez ces cadres(frames ou images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "KMcngIsad3Mu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for simulation: 219\n",
      "Timesteps taken: 471\n",
      "Penalties incurred: 149\n"
     ]
    }
   ],
   "source": [
    "# TODO: Retrieve the corresponding frames from running the simulation starting from the state found in Q1 (b). Then show those frames.\n",
    "state = 219\n",
    "frames, _, _ = run_single_simulation_baseline(env, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwgxcH_Ojfud"
   },
   "source": [
    "Avec la possibilité de simuler des exécutions uniques d'un épisode avec l'approche de base, nous allons maintenant définir une fonction que nous utiliserons pour évaluer les performances générales du modèle de base lorsqu'il est exécuté sur plusieurs épisodes. La fonction *evaluate_agent_baseline* ci-dessous accepte comme entrée le nombre total d'épisodes sélectionnés au hasard à exécuter avec l'environnement, exécute les épisodes aléatoires, affiche le nombre moyen de pas de temps pris par épisode avec les pénalités moyennes encourues et renvoie les données d'image.\n",
    "\n",
    "***Si la fonction evaluate_agent_baseline semble s'exécuter trop longtemps (plusieurs minutes, pas une seule), arrêtez l'exécution en cliquant sur le bouton en haut à gauche de la cellule de code en cours d'exécution et réexécutez-la.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "7GLAT5_mjgEu"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_baseline(episodes, env):\n",
    "    '''\n",
    "    Given a number of episodes and an environment, run the specified\n",
    "    number of episodes, where each run begins with a random state, display the\n",
    "    naverage timesteps per episode and the average penalties per episode, and output\n",
    "    the frames to be displayed.\n",
    "    '''\n",
    "    total_time_steps, total_penalties = 0, 0\n",
    "    frames = []\n",
    "    # Run through the total number of episodes\n",
    "    for _ in range(episodes):\n",
    "        # Get a random state\n",
    "        state = env.reset()\n",
    "        # Run the simulation, obtaining the results\n",
    "        frame_data, penalties, time_steps = run_single_simulation_baseline(env, state, True)\n",
    "        # Update the tracked data over all simulations\n",
    "        total_penalties += penalties\n",
    "        total_time_steps += time_steps\n",
    "        frames = frames + frame_data\n",
    "    print(f\"Results after {episodes} episodes:\")\n",
    "    print(f\"Average timesteps per episode: {total_time_steps / episodes}\")\n",
    "    print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHkepDd2C3im"
   },
   "source": [
    "**(TO DO) Q3**\n",
    "\n",
    "a) Utilisez la fonction *evaluate_agent_baseline* définie ci-dessus pour exécuter 100 épisodes aléatoires pour l'environnement.\n",
    "\n",
    "b) D'après les résultats de la Q3 (a), comment l'approche de référence a-t-elle fonctionné et pourquoi pensez-vous qu'elle a bien ou mal fonctionné? Expliquez les temps moyens par épisode et les pénalités moyennes par épisode.\n",
    "\n",
    "c) Sans passer à une approche d'apprentissage par renforcement, comment l'approche de base peut-elle être modifiée pour être légèrement meilleure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3Tnw6bBxMXv"
   },
   "source": [
    "**(TO DO) Q3 (a) - 1 point**\n",
    "\n",
    "a) Utilisez la fonction *evaluate_agent_baseline* définie ci-dessus pour exécuter 100 épisodes aléatoires pour l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "BPRemQq-xXUT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 2512.88\n",
      "Average penalties per episode: 807.92\n"
     ]
    }
   ],
   "source": [
    "# TODO ...\n",
    "frames = evaluate_agent_baseline(100,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dClJX6AWxZM5"
   },
   "source": [
    "**(TO DO) Q3 (b) - 1 point**\n",
    "\n",
    "b) D'après les résultats de la Q3 (a), comment l'approche de référence a-t-elle fonctionné et pourquoi pensez-vous qu'elle a bien ou mal fonctionné? Expliquez les temps moyens par épisode et les pénalités moyennes par épisode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxUTqzCSxhZU"
   },
   "source": [
    "TODO ...     \n",
    "Par reference l'action et choisi de facon aleatoire en fonction de l'etat actuer et les possibiliter d'effectuer une action qui a deja ete prise et a conduit a une penalite, cela ce produit tant que la destination final n'est pas atteinte. Le temps moyen par episode correspond au nombre de deplacement moyen par le taxi pour recuper et deposer le passager a la destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Wr3F6IExrQQ"
   },
   "source": [
    "**(TO DO) Q3 (c) - 1 point**\n",
    "\n",
    "c) Sans passer à une approche d'apprentissage par renforcement, comment l'approche de base peut-elle être modifiée pour être légèrement meilleure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_b7h54SDQ7C"
   },
   "source": [
    "TODO ...  L'approche pourrait etre legerement meilleur en sauvegardant les deplacements que le taxi effectue, pour faire un deplacement seulement s'il na pas deja ete effectuer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GfrTVl3AuhD"
   },
   "source": [
    "**3.0 - Entraîner un agent avec Q-Learning pour jouer au jeu du taxi**   \n",
    "\n",
    "Maintenant que nous avons eu un agent utilisant l'approche de base pour compléter la simulation de taxi, nous demanderons à l'agent d'utiliser Q-Learning pour essayer d'appliquer une approche d'apprentissage par renforcement au problème. Pour démarrer le processus, nous allons créer une matrice de valeurs Q pour chaque possibilité d'état d'action (en l'initialisant à zéro). L'agent mettra à jour cette matrice lors de l'entraînement et aura besoin de la réinitialiser chaque fois qu'il souhaite réinitialiser son entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "IDTbzsaEanoU"
   },
   "outputs": [],
   "source": [
    "# Initialize the table of Q values for the state-action pairs\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mph6_GdSGy2F"
   },
   "source": [
    "Une fois la matrice des valeurs Q initialisée, nous allons maintenant définir la fonction d'apprentissage qui ajuste les valeurs Q dans *q_table*. Le processus d'apprentissage consiste à exécuter un certain nombre de simulations aléatoires et à mettre à jour les valeurs Q pour chaque état via Q-Learning.\n",
    "\n",
    "Il existe un certain nombre d'hyperparamètres utilisés par la fonction d'apprentissage:\n",
    "\n",
    "- *alpha*: paramètre d'apprentissage (vous devrez le décrire dans une question ultérieure).\n",
    "- *gamma*: le paramètre de remise de récompense à long terme.\n",
    "- *epsilon*: paramètre d'Exploitation/Exploration (vous devrez le décrire dans une question ultérieure).\n",
    "- *num_simulations*: Représente le nombre d'épisodes aléatoires à générer pour que les agents les utilisent pour mettre à jour ses valeurs Q.\n",
    "\n",
    "Ainsi, en exécutant cet algorithme, un agent peut apprendre les valeurs Q à utiliser lorsqu'il travaille avec d'autres épisodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "6kbLhT-Ua0Cc"
   },
   "outputs": [],
   "source": [
    "def train_agent(alpha, gamma, epsilon, num_simulations):\n",
    "    '''\n",
    "    Trains an agent by updating its Q values for a total of num_simulations\n",
    "    episodes with the alpha, gamma, and epsilon hyperparameters. \n",
    "    '''\n",
    "    # For plotting metrics\n",
    "    all_time_steps = []\n",
    "    all_penalties = []\n",
    "    # Generate the specified number of episodes\n",
    "    for i in range(1, num_simulations + 1):\n",
    "        # Generate a new state by resetting it\n",
    "        state = env.reset()\n",
    "        # Variables tracked (time steps, total penalties, the reward value)\n",
    "        time_steps, penalties, reward, = 0, 0, 0\n",
    "        done = False\n",
    "        # Run the simulation \n",
    "        while not done:\n",
    "            # Select a random action is the randomly selected number from a\n",
    "            # uniform distribution is less than epsilon\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample() # Explore action space\n",
    "            # Otherwise use the currently learned Q values\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Exploit learned values\n",
    "            # Retrieve the relevant information after performing the action\n",
    "            next_state, reward, done, info = env.step(action) \n",
    "            # Retrieve the old Q value and the maximum Q value from the next state\n",
    "            old_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            # Update the current Q value\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "            # Track anytime an incorrect dropoff or pickup is made\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "            # Proceed to the next state and time step\n",
    "            state = next_state\n",
    "            time_steps += 1\n",
    "        # Display progress for each 100 episodes\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode: {i}\")\n",
    "    print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLoNggkqQu2L"
   },
   "source": [
    "Nous utilisons maintenant la fonction d'entraînement avec un ensemble d'hyperparamètres pour entraîner l'agent avec Q-Learning afin d'améliorer potentiellement les performances au fil du temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "_tkJvaTwPjfB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.5\n",
    "epsilon = 0.1\n",
    "num_simulations = 100000\n",
    "# Train the agent\n",
    "train_agent(alpha, gamma, epsilon, num_simulations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmzQceEvRyT3"
   },
   "source": [
    "Après l'entraînement, nous pouvons regarder les valeurs Q qui ont été obtenues dans notre table state-action pour un état spécifique. Ci-dessous, nous voyons que chaque valeur Q pour les six actions possibles disponibles pour l'état 328 a été mise à jour en conséquence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvXOzHtVAar6"
   },
   "source": [
    "**(TO DO) Q4 - 2 points**\n",
    "\n",
    "Ci-dessous, nous imprimons les valeurs Q disponibles pour les six actions à l'état 328 et nous rendons(venant de render)/montrons cet état pour le visualiser. Sur la base des valeurs Q disponibles (en supposant que nous sommes en mode d'exploitation), quelle action serait la prochaine à être sélectionnée (ou s'il y a des égalités, listez toutes les actions possibles qui seraient envisagées)? Certaines des actions contenant des valeurs Q plus élevées semblent-elles problématiques si jamais elles sont sélectionnées? Pourquoi ou pourquoi pas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "-KP5FMBQa1el"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.98513267  -1.95703125  -1.98408534  -1.97698365 -10.25056685\n",
      " -10.58847597]\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "print(q_table[328])\n",
    "env.s = 328\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xx4axJNOBl_0"
   },
   "source": [
    "TODO ...    \n",
    "La prochaine valeur selectionner vers le nord n'est pas problematique car les valeurs sont choisi du minimum vers le plus grand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncwGTV5PSHZx"
   },
   "source": [
    "Une fois l'entraînement terminé, nous pouvons maintenant évaluer l'approche Q-Learning dans une méthode similaire que nous avons utilisée pour évaluer l'approche de base. En passant le nombre d'épisodes à tester et l'environnement, nous générons ce nombre d'épisodes aléatoires et faisons la moyenne des résultats obtenus en exécutant l'approche Q-Learning pour terminer les épisodes. Contrairement à l'entraînement, il est important de noter que les hyperparamètres qui y sont utilisés ne sont pas utilisés ici. L'agent utilise simplement la valeur Q maximale à chaque étape pour déterminer quelle action entreprendre à un pas de temps donné.\n",
    "\n",
    "***Si la fonction evaluate_agent_QL semble s'exécuter trop longtemps (une minute ou plus), arrêtez l'exécution en cliquant sur le bouton en haut à gauche de la cellule de code en cours d'exécution et réexécutez-la. Cela se produit parce que l'entraînement était suffisant pour définir des valeurs Q valides et a entraîné une impasse pour un état spécifique.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "CuitaI8Ra2u8"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_QL(episodes, env):\n",
    "    '''\n",
    "    Given a number to specify how many random states to run and the environment to use,\n",
    "    display the averaged metrics obtained from the tests and return the frames obtained from the tests.\n",
    "    '''\n",
    "    total_time_steps, total_penalties = 0, 0\n",
    "    frames = []\n",
    "    for _ in range(episodes):\n",
    "        # Generate a random state to use\n",
    "        state = env.reset()\n",
    "        # The information collected throughout the run\n",
    "        time_steps, penalties, reward = 0, 0, 0\n",
    "        # Determines when the episode is complete\n",
    "        done = False\n",
    "        # Run through the episode until complete\n",
    "        while not done:\n",
    "            # Select the action containing the maximum Q value\n",
    "            action = np.argmax(q_table[state])\n",
    "            # Run that action and retrieve the reward and other details\n",
    "            state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Put each rendered frame into dict for animation\n",
    "            frames.append({\n",
    "                'frame': env.render(mode='ansi'),\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward\n",
    "                }\n",
    "            )\n",
    "            # Specify whether the agent incorrectly chose to pick up or dropoff a passenger\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "            # Increment the current time step\n",
    "            time_steps += 1\n",
    "        # Track the totals\n",
    "        total_penalties += penalties\n",
    "        total_time_steps += time_steps\n",
    "    # Display the performance over the tests\n",
    "    print(f\"Results after {episodes} episodes:\")\n",
    "    print(f\"Average timesteps per episode: {total_time_steps / episodes}\")\n",
    "    print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
    "    # Return the frames to allow a user to view the runs\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEQNpWoSF822"
   },
   "source": [
    "**(TO DO) Q5**\n",
    "\n",
    "a) Exécutez *evaluate_agent_QL* pendant 100 épisodes pour récupérer le nombre moyen de pas de temps et la pénalité moyenne après l'entraînement.\n",
    "\n",
    "b) Compte tenu de vos résultats de Q5 (a), comment les résultats observés des tests se comparent-ils aux tests de l'approche de base de Q3 (a)? Plus précisément, quel agent est le plus performant par rapport au nombre moyen de pénalités tout au long des tests et quel agent est capable de résoudre les problèmes plus rapidement (en moyenne)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TzyRekwGolK"
   },
   "source": [
    "**(TO DO) Q5 (a) - 1 point**\n",
    "\n",
    "a) Exécutez *evaluate_agent_QL* pendant 100 épisodes pour récupérer le nombre moyen de pas de temps et la pénalité moyenne après l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "lAliG8sjTzBc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.93\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "# TODO ...\n",
    "frames = evaluate_agent_QL(100,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_Os_s3UGxK2"
   },
   "source": [
    "**(TO DO) Q5 (b) - 2 points**\n",
    "\n",
    "b) Compte tenu de vos résultats de Q5 (a), comment les résultats observés des tests se comparent-ils aux tests de l'approche de base de Q3 (a)? Plus précisément, quel agent est le plus performant par rapport au nombre moyen de pénalités tout au long des tests et quel agent est capable de résoudre les problèmes plus rapidement (en moyenne)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPjrUlM4G_f_"
   },
   "source": [
    "TODO ... \n",
    "\n",
    "On peut observer avec les rusultat que QL est plus performant que baseline car on obtient moins de penalites pour chaques episode et cela prend moins de temps par episoder pour prendre et deposer le passager a la destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woAhj7vqaD5q"
   },
   "source": [
    "**4.0 - Test de différents hyperparamètres**   \n",
    "\n",
    "Nous allons maintenant essayer de re-entraîner l'agent en utilisant différentes configurations avec les hyperparamètres. Cela vous permettra d'explorer leur impact sur le Q-Learning et de comprendre leur objectif pendant l'entraînement.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBamDNUIZ0Iy"
   },
   "source": [
    "**(TO DO) Q6**     \n",
    "Ci-dessous, nous explorons différentes variations de quatre hyperparamètres utilisés par l'approche Q-Learning pour mieux comprendre leur impact sur l'entraînement. Lorsque vous répondez aux questions, ***veillez à régler correctement les hyperparamètres***.\n",
    "\n",
    "a) Re-entraînez l'agent en réinitialisant les valeurs d'apprentissage Q et l'entraînement pour seulement **35000 épisodes** (avec les mêmes valeurs alpha, gamma et epsilon utilisées dans la section 3.0 de ce notebook). Ensuite, effectuez un autre test pour 100 épisodes dans l'environnement.\n",
    "\n",
    "b) Re-entraînez l'agent en réinitialisant les valeurs d'apprentissage Q et en s'entraînant pour **100000 épisodes**, mais avec une **valeur epsilon de 0,8** (avec les mêmes valeurs alpha et gamma utilisées dans la section 3.0 de ce notebook). Ensuite, effectuez un autre test pour 100 épisodes dans l'environnement.\n",
    "\n",
    "c) Re-entraînez l'agent en réinitialisant les valeurs d'apprentissage Q et en s'entraînant pour **100000 épisodes**, mais avec une **valeur alpha de 0,7** (avec les mêmes valeurs gamma et epsilon utilisées dans la section 3.0 de ce notebook). Ensuite, effectuez un autre test pour 100 épisodes dans l'environnement.\n",
    "\n",
    "d) Re-entraînez l'agent en réinitialisant les valeurs d'apprentissage Q et en entraînant pour **100000 épisodes**, mais avec une **valeur gamma de 0,15** (avec les mêmes valeurs alpha et epsilon utilisées dans la section 3.0 de ce notebook). Ensuite, effectuez un autre test pour 100 épisodes dans l'environnement.\n",
    "\n",
    "e) En vous basant sur vos connaissances, décrivez quelles sont les rôles des valeurs alpha et epsilon dans la fonction d'entraînement (c'est-à-dire qu'est-ce qu'elles font/leurs impacts).\n",
    "\n",
    "f) En utilisant les résultats obtenus à partir de vos tests en Q6 (a), (b), (c) et (d), ainsi que les résultats initiaux de Q5, expliquez les impacts de la modification du nombre d'épisodes entraînés (moins d'épisodes vs plus d'épisodes), la valeur alpha (petite vs grande), la valeur gamma (petite vs grande) et la valeur epsilon (petite vs grande). Même si les différences dans les comparaisons sont mineures, indiquez-les.\n",
    "\n",
    "A noter, ci-dessous sont les valeurs d'hyperparamètres initiales utilisées à la section 3.0 de ce notebook à utiliser comme référence:\n",
    "\n",
    "*alpha* = 0,1\n",
    "\n",
    "*gamma* = 0,5\n",
    "\n",
    "*epsilon* = 0,1\n",
    "\n",
    "*num_simulations* = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQxxvn0xaxrC"
   },
   "source": [
    "**(TO DO) Q6 (a) - 2 points**\n",
    "\n",
    "a) Re-entraînez l'agent en réinitialisant les valeurs d'apprentissage Q et l'entraînement pour seulement **35000 épisodes** (avec les mêmes valeurs alpha, gamma et epsilon utilisées dans la section 3.0 de ce notebook). Ensuite, effectuez un autre test pour 100 épisodes dans l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "sxQbWXNha4EP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 35000\n",
      "Training finished.\n",
      "\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 13.51\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Reset q_table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "# TODO: Retrain with the specified hyperparameters\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.5\n",
    "epsilon = 0.1\n",
    "num_simulations = 35000\n",
    "train_agent(alpha, gamma, epsilon, num_simulations)\n",
    "# TODO: Test for 100 episodes\n",
    "frames=evaluate_agent_QL(100,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kSCmM6VFdHu"
   },
   "source": [
    "**(TO DO) Q6 (b) - 2 points**\n",
    "\n",
    "b) Re-entraînez l'agent en réinitialisant les valeurs d'apprentissage Q et en s'entraînant pour **100000 épisodes**, mais avec une **valeur epsilon de 0,8** (avec les mêmes valeurs alpha et gamma utilisées dans la section 3.0 de ce notebook). Ensuite, effectuez un autre test pour 100 épisodes dans l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "id": "O0vyZ7TNFeF7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 13.32\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Reset q_table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "# TODO: Retrain with the specified hyperparameters\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.5\n",
    "epsilon = 0.8\n",
    "num_simulations = 100000 \n",
    "train_agent(alpha, gamma, epsilon, num_simulations)\n",
    "# TODO: Test for 100 episodes\n",
    "frames=evaluate_agent_QL(100,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWcn_doxFm4H"
   },
   "source": [
    "**(TO DO) Q6 (c) - 2 points**\n",
    "\n",
    "c) Re-entraînez l'agent en réinitialisant les valeurs d'apprentissage Q et en s'entraînant pour **100000 épisodes**, mais avec une **valeur alpha de 0,7** (avec les mêmes valeurs gamma et epsilon utilisées dans la section 3.0 de ce notebook). Ensuite, effectuez un autre test pour 100 épisodes dans l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "kSM0Ert2FnHP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.82\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Reset q_table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "# TODO: Retrain with the specified hyperparameters\n",
    "# Hyperparameters\n",
    "alpha = 0.7\n",
    "gamma = 0.5\n",
    "epsilon = 0.1\n",
    "num_simulations = 100000\n",
    "train_agent(alpha, gamma, epsilon, num_simulations)\n",
    "# TODO: Test for 100 episodes\n",
    "frames=evaluate_agent_QL(100,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZwH8XogFreB"
   },
   "source": [
    "**(TO DO) Q6 (d) - 2 points**\n",
    "\n",
    "d) Re-entraînez l'agent en réinitialisant les valeurs d'apprentissage Q et en entraînant pour **100000 épisodes**, mais avec une **valeur gamma de 0,15** (avec les mêmes valeurs alpha et epsilon utilisées dans la section 3.0 de ce notebook). Ensuite, effectuez un autre test pour 100 épisodes dans l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "id": "AVFGLvcZFrDj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.42\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Reset q_table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "# TODO: Retrain with the specified hyperparameters\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.15\n",
    "epsilon = 0.1\n",
    "num_simulations = 100000\n",
    "train_agent(alpha, gamma, epsilon, num_simulations)\n",
    "# TODO: Test for 100 episodes\n",
    "frames=evaluate_agent_QL(100,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YY-Vgmn8GQ_9"
   },
   "source": [
    "**(TO DO) Q6 (e) - 2 points**\n",
    "\n",
    "e) En vous basant sur vos connaissances, décrivez quelles sont les rôles des valeurs alpha et epsilon dans la fonction d'entraînement (c'est-à-dire qu'est-ce qu'elles font/leurs impacts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro9AnLRhGRtV"
   },
   "source": [
    "TODO ...  \n",
    "\n",
    "Nous remarquons que le taux au quel l'agent executes les nouvelles action est representer par epsillon. Alors que alpha represente entre la valeur de Q courante et la valeur de Q de l'etat suivant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ec4qnmyJbWYw"
   },
   "source": [
    "**(TO DO) Q6 (f) - 4 points**\n",
    "\n",
    "f) En utilisant les résultats obtenus à partir de vos tests en Q6 (a), (b), (c) et (d), ainsi que les résultats initiaux de Q5, expliquez les impacts de la modification du nombre d'épisodes entraînés (moins d'épisodes vs plus d'épisodes), la valeur alpha (petite vs grande), la valeur gamma (petite vs grande) et la valeur epsilon (petite vs grande). Même si les différences dans les comparaisons sont mineures, indiquez-les."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WA3mYBjObbMi"
   },
   "source": [
    "TODO ...   Nous remarquons que les difference dans le temps par episode varrient tres legerement quand le nombre d'episode et le meme entre Q5 et Q6 avec un legere avantage de l'approche Q5 sauf quand on on reduit les valeur de alpha ou gamma a ce moment la le temps par episode et plus interessant dans Q6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiD_vGE6jMOu"
   },
   "source": [
    "***SIGNATURE:***\n",
    "Mon nom est Valentin Magot.\n",
    "Mon numéro d'étudiant est 8843488\n",
    "Je confirme être l'auteur de ce travail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CSI4506_RL_Fall20.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
