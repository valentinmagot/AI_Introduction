{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CkFQVsf7hBV"
   },
   "source": [
    "# Notebook 9 - GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5PCrI7a7hBW"
   },
   "source": [
    "CSI4506 Intelligence Artificielle   \n",
    "Automne 2020  \n",
    "Preparé par Julian Templeton, Caroline Barrière et Joel Muteba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edd5V5Zy7hBW"
   },
   "source": [
    "***INTRODUCTION (Lisez ceci attentivement!)***:\n",
    "\n",
    "Avec la résurgence de l'apprentissage en profondeur (Deep Learning) et des architectures d'apprentissage en profondeur (Deep Learning Architectures, DLA) en raison de la puissance accrue de l'informatique moderne, il existe une multitude de façons dont les DLA peuvent être utilisés pour s'attaquer à pratiquement tous les types de problèmes (la reconnaissance faciale, les prévisions boursières, la génération d'images, ...).\n",
    "\n",
    "La génération de fausses images, vidéos, musique et textes (ex: poèmes) a été un sujet d'intérêt dans la société. Ce sont des problèmes avec des applications pratiques, comme dans les films, mais peuvent également avoir des implications éthiques importantes. Cela dit, ces DLA sont très intéressants à utiliser et peuvent être facilement implémentés grâce à l'utilisation de bibliothèques Python Deep Learning telles que [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/) et [Keras](https://keras.io/). Cela dit, bien qu'il soit possible de trouver de nombreux exemples d'utilisation de ces DLA, ils nécessitent toujours une immense puissance de calcul pour fournir des résultats efficaces.\n",
    "\n",
    "Dans ce notebook, nous explorerons une tâche commune de génération d'images grâce à l'utilisation d'un réseau antagoniste génératif (GAN). En commençant par des images générées aléatoirement comme celle-ci    \n",
    "\n",
    "\n",
    "\n",
    "![RandomNoise.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZD0lEQVR4nO3de3DU1d0G8OcrhIvhGrnFkHIrYqlVwagUEEO9QG1HoK0WhiqiFae1jnjDWtvRMp2p9QZOpVCw4A2lKlKpMgpSBfECBEQBAQUaBATCRRCCXELO+0fWvlFznpPuht193/N8ZpiEffLd/WU33+xmz++cY845iMj/fydk+gBEJD3U7CKRULOLRELNLhIJNbtIJOqn88Zyc3NdXl6eNz927Bitr1evXtK3bWY0P3z4MM0bN27szSoqKmht6LhTHRE5evSoNwt93yGh+srKSprn5OR4s9D9FrrtUM5+nho2bEhrQ99XSOgxZdcf+r7Yde/duxfl5eU1XkFKzW5mAwE8BKAegEecc/ewr8/Ly8PNN99MD5Rp2bJlEkdZJdRwGzZsoPnpp5/uzcrKymhtixYtaB76oQ/Zvn27Nwv94ITul/r1+Y/IoUOHaN6uXTtvtnPnTlrLflEA4Yb99NNPvVmXLl1o7cGDB2keauYjR47QnN1voe+L1U6aNMmbJf0y3szqAZgA4PsAugMYZmbdk70+ETm+Uvmb/RwA651zG51zRwDMADCobg5LROpaKs1eAGBztf9vSVz2JWY2ysxKzKykvLw8hZsTkVQc93fjnXOTnXNFzrmi3Nzc431zIuKRSrNvBVBY7f/tE5eJSBZKpdmXAuhqZp3MrAGAoQBm181hiUhdS3rozTlXYWa/AvAKqobepjrnVrOayspKsL/bTzrppNBterNGjRrR2tAwz89+9jOaT506Nena119/neY9e/ak+erV9G5Ft27dvNmCBQto7U9+8hOav/zyyzS/+OKLaT57tv/3/1VXXUVrFy5cSHN2zgbAz5347LPPaO1bb71F89DQXadOnWjeo0cPbzZjxgxa279/f2/Ghu1SGmd3zs0BMCeV6xCR9NDpsiKRULOLRELNLhIJNbtIJNTsIpFQs4tEIq3z2evVq4dmzZp589DYJ5tuGRpnD42LhsZVW7duTXOGzTcHgJUrV9J806ZNNGdTXEPnLqxdu5bm7PECgM2bN9O8uLjYm5WWltLa999/n+YDBw6kOZtm2qBBA1p72mmn0bxjx440/+STT2i+ceNGbxaa2nvgwAFvxubw65ldJBJqdpFIqNlFIqFmF4mEml0kEmp2kUikdegN4KudhoZievXqlXQtW+UUCC9j3bVrV2+2Zs0aWrt+/Xqas2WqASA/P5/m7HsLTY8NXXdoem5o9aF77vEvODxy5EhaG5omunv3bprv37/fmy1ZsoTW/vCHP6T5q6++SvPQqrxsaK537960dtWqVd6MDTfqmV0kEmp2kUio2UUioWYXiYSaXSQSanaRSKjZRSKR1nH20FLShYWF3gzg01ibNm1Ka9mUwtrUt2rVypu1adOG1oaceOKJNA/t6tmhQwdv9uabb9LadevW0bxz5840P/nkk2nOjq2g4Gu7hX3Jjh07aN69O99HdP78+d6sb9++tDa0Vdl5551H82nTptF88ODB3uxf//oXrWXnfLBdefXMLhIJNbtIJNTsIpFQs4tEQs0uEgk1u0gk1OwikUjrOHtOTg4dWw0tmczmrPfr14/WhrZsZvOAAeCJJ57wZqGx5tBc+SuuuILmjz/+OM2feuopbzZx4kRa+/zzz9N87969NL/99ttpPnfuXG9233330drQ+QdsfQOALwcdGsMPzUcPnSMwYcIEmo8YMSLp627fvr03q6ys9GYpNbuZlQLYD+AYgArnXFEq1ycix09dPLP3d87tqoPrEZHjSH+zi0Qi1WZ3AOaa2TIzG1XTF5jZKDMrMbMStiaYiBxfqb6M7+uc22pmbQDMM7O1zrmF1b/AOTcZwGQA6NSpk0vx9kQkSSk9szvntiY+lgGYBeCcujgoEal7STe7meWaWdMvPgdwMQD/GrciklGpvIxvC2BWYh34+gCecs69zAqOHj1KtxdmY4QAXwf8xRdfpLWhsfDQ9sBsLe/Qlsxt27aleWgN8j179tB8yJAh3uyvf/0rrQ2dXzBu3Diav/HGGzR/+OGHvdno0aNp7cKFC2m+YsUKmh86dMibhdbLD53zUVZWRvOQv/zlL97spZdeorXs/AG2B0HSze6c2wjgjGTrRSS9NPQmEgk1u0gk1OwikVCzi0RCzS4SibROca1fvz5atGjhzVPZunj58uW0Ni8vj+Zjxoyh+QsvvODNQkNrjzzyCM1bt25N87PPPpvmM2bM8GaXXXYZrf30009pPnbsWJqHpmN+/vnn3iy0HXToMQsNSTLFxcU0Z9OGayO0HPTw4cO92ccff0xr2ZDigQMHvJme2UUioWYXiYSaXSQSanaRSKjZRSKhZheJhJpdJBJpHWc/fPgwnTp4yimn0Po1a9Z4s9CSV7t28TUx58yZQ/N3333Xm/Xo0YPWdurUieYjR46keWgK7emnn+7NHn30UVrLxmyB8FTQefPm0ZxNHT7ppJNobWgcnY0pA8DWrVu92bPPPktrQ1N/mzdvTvP33nuP5gcPHvRmoa2o2RLbDRo08GZ6ZheJhJpdJBJqdpFIqNlFIqFmF4mEml0kEmp2kUikfcvmNm3aePNt27bR+j59+nizW2+9ldYmlrz2Cs37Zlv8XnfddbR248aNNL/rrrtofuqpp9KcjbvOnDmT1i5YsIDm06dPpzlbGhwAZs+e7c1Cj1lo+W+2HTQAXHrppd6sY8eOtPbtt9+m+YYNG2h+7bXX0pz9PL7yyiu0li1jzdYn0DO7SCTU7CKRULOLRELNLhIJNbtIJNTsIpFQs4tEIq3j7M45Ojc7tHXxzp07vdkvfvELWtu5c2eah+Zlf/vb3/ZmbAweCK/NPmjQIJrv27eP5oWFhd7slltuobXt2rWj+erVq2nO9gEA+HhzaL39pk2b0vymm26ieUlJiTe78MILae2yZcto3qFDB5qHtpNmaxwMGDCA1rL1C66++mpvFnxmN7OpZlZmZquqXZZnZvPM7KPEx5ah6xGRzKrNy/hHAQz8ymW/BjDfOdcVwPzE/0UkiwWb3Tm3EMBX1wcaBOCxxOePARhcx8clInUs2Tfo2jrnvjiRfTsA7x9fZjbKzErMrKS8vDzJmxORVKX8brxzzgFwJJ/snCtyzhXl5uamenMikqRkm32HmeUDQOKjfxqOiGSFZJt9NoARic9HAPDvZywiWSE4zm5mTwMoBtDKzLYAuAvAPQCeMbNrAGwCcHltbszM0LBhQ28+ZMgQWs/WxF67di2tbdSoEc03b95M82984xtJHRcALF26lOalpaU079evH83Zft6hsep//OMfNP/pT39K81WrVtH8/PPP92ahtf7bt29P8/79+9OcnSMQekzYzykAPPnkkzQPnTuxZcsWbxaax89+Xg4fPuzNgs3unBvmiS4I1YpI9tDpsiKRULOLRELNLhIJNbtIJNTsIpFI6xTXyspKulVtaAvfY8eOebOcnBxa+9FHH9H8hhtuoDmbfhtaSjo0hLR3716at2rViubTpk3zZgUFBbT2d7/7Hc1DQ2tsSBLgS1k3a9aM1oa2ug5tR/3cc895s/Hjx9PaiooKmv/4xz+m+eLFi2nOtvlet24drc3Ly/NmbAq5ntlFIqFmF4mEml0kEmp2kUio2UUioWYXiYSaXSQSaR1nb9SoEb75zW9689B49Jw5c7zZBRfwSXihrYWXLFlCcya0VPSf/vQnmoemeo4dO5bmbEvn2267jdY+88wzNA9NHQ6dv/Dggw96s0WLFtHa0DkAffv2pfn69eu92fDhw2ktWzocAIqKimjevHlzmrNltF977TVau3DhQm924MABb6ZndpFIqNlFIqFmF4mEml0kEmp2kUio2UUioWYXiYRVbeiSHoWFhW706NHePDS+eNZZZ3mz0Jzv0G40oW2Xt27d6s1C87LZODgAfOtb36L5rFmzaH7CCf7f2aHvO/T4h7bRvuOOO2h+5ZVXerMRI0Z4MwD4/PPPaR4a63733Xe9GVsbAQC6detGc7YuAwB8+OGHNB8zZow3C20XXb++//SYMWPGYMOGDVZTpmd2kUio2UUioWYXiYSaXSQSanaRSKjZRSKhZheJRFrns5sZXd+9e/futH7nzp3e7Dvf+Q6tnTdvHs13795NczbXPj8/n9aG1l4PrXnfunVrmrPx5jVr1tDa8vJymv/yl7+k+UUXXURz9riE1hAIbYs8depUmvfp08ebde7cmdZOmTKF5qFzI0Lj7H/4wx+8WYsWLWgt26J7z5493iz4zG5mU82szMxWVbvsbjPbamYrEv8uCV2PiGRWbV7GPwpgYA2Xj3POnZn4519CRkSyQrDZnXMLAfhfG4jI/wmpvEH3KzN7P/Eyv6Xvi8xslJmVmFkJWx9LRI6vZJt9IoAuAM4EsA3AA74vdM5Nds4VOeeKmjRpkuTNiUiqkmp259wO59wx51wlgCkAzqnbwxKRupZUs5tZ9bGmIQD42JKIZFxwnN3MngZQDKCVmW0BcBeAYjM7E4ADUAqAb1BeDZs/HfqbvmnTpt5sxYoVtDa093toj/V//vOf3mzfvn20NjQv+7PPPqN5aG33uXPnerM2bdrQWrZPOACUlZXRnO0NHzJkyBCah8b4f/SjH9H8zTff9GabNm2itYWFhTQP1bM91AF+v4YeE7Z2A9sXPtjszrlhNVz8t1CdiGQXnS4rEgk1u0gk1OwikVCzi0RCzS4SibROcQ3p1asXzdlyzmwraAD497//TXOzGlff/Q82ZBjaOjg0pFhaWkrzc889l+Zsa+KJEyfSWrbUMxCevjtp0iSaP//8896MDWcC4SW47733Xpo//PDD3mzy5Mm0NjT09sknn9D8t7/9Lc2fe+45bxYaJmaPSaNGjbyZntlFIqFmF4mEml0kEmp2kUio2UUioWYXiYSaXSQSad2yOT8/340cOdKbN27cmNaz8eTi4mJau2vXLpqHpnK+88473mzAgAG0NjSFNTRmGzpHgG2DXVFRkdJ1h7ZsDo3Ts3MMQtNEQ0tsX3755TT/4x//6M3Y9t8AcOTIEZqzZc0BoF69ejRv3ry5Nzt06BCtZffbK6+8gt27d2vLZpGYqdlFIqFmF4mEml0kEmp2kUio2UUioWYXiURa57PXr1+fLrF78OBBWn/aaad5s2bNmtHat99+m+ahrYfZ+QHjx4+ntSGDBg2iOVsqOpSz8VwgfH5B//79af7666/TnG0v/Pvf/57WhtYYeOAB70ZEAIC2bdt6s3PO4fuahJbvZttkA+GfZTYOv3TpUlrbrl07b1a/vr+l9cwuEgk1u0gk1OwikVCzi0RCzS4SCTW7SCTU7CKRSOs4u5nRda1Da7+zed9sO2cA2Lt3L8137NhB82effdab9ezZk9aG5pR36dKF5kOHDqX5lClTvFlozfrLLruM5h9++CHN2Tg6AFx99dXe7MUXX6S1d955J83PP/98mr/00kvebO3atbR22LCaNi/+Xw899BDNQz/LbI2D0Bg+e0zZGgDBZ3YzKzSz18zsAzNbbWY3Ji7PM7N5ZvZR4mPL0HWJSObU5mV8BYBbnHPdAfQCcL2ZdQfwawDznXNdAcxP/F9EslSw2Z1z25xzyxOf7wewBkABgEEAHkt82WMABh+vgxSR1P1Xb9CZWUcAPQAsBtDWObctEW0HUOOJyGY2ysxKzKwk9PejiBw/tW52M2sCYCaA0c65L7274KpWraxx5Urn3GTnXJFzrqhJkyYpHayIJK9WzW5mOahq9OnOuS+25dxhZvmJPB8Anz4lIhkVHHqzqnmGfwOwxjn3YLVoNoARAO5JfHyhNjfIlq6eNWsWre3Tp483u//++2lt7969aV5ZWUlzNkQ1bdo0WhtaVnjgwIE0X7x4Mc337dvnzW677TZau3LlSpqHhixDU4vZdtRXXHEFrWXbPQPAuHHjaM6m0IZ+HhYsWEDz0DbbF154Ic23bNnizaZPn05r+/Xr582OHTvmzWozzt4HwBUAVprZisRlv0FVkz9jZtcA2ASAL+ItIhkVbHbn3CIAvlUELqjbwxGR40Wny4pEQs0uEgk1u0gk1OwikVCzi0QirVs2FxQUuOuuu86bh8Z0Wc7G4IHw0r6hcXa2fG+vXr1o7cSJE2m+bNkyml9wAR/0aN++vTfbuHEjrW3QoAHNZ86cSfMRI0bQ/Omnn/ZmZ599Nq0NCU0d3r17tzfr1q0brT18+DDNQ9t0T5gwIen6Dz74gNayJdXHjh2L0tJSbdksEjM1u0gk1OwikVCzi0RCzS4SCTW7SCTU7CKRSOtS0jk5OSgsLPTmhw4dovWDB/uXuQtta7x+/XqaN2zYkOaXXHKJN7vpppto7TvvvEPzq666iub5+fk0X758uTfbs2cPrT311FNp3rVrV5qHtoS+5pprvFloPvodd9xB8yVLltCcLXNdXFxMa0Pz9ENzzr/73e/SfN26dd7se9/7Hq1ly3uz+ex6ZheJhJpdJBJqdpFIqNlFIqFmF4mEml0kEmp2kUikdZy9srIS5eXl3jw0v7mkpMSbFRQU0NrQ1lPnnnsuzdlW0+eddx6tDY3Zhm6bjaMDQMeOHb3ZySefTGtD5xeE7teWLfnmvX/+85+92Q9+8ANae/ToUZqfcsopNGfbIofWTght6Rza6vraa6+l+aWXXurNQttgs63L2X2mZ3aRSKjZRSKhZheJhJpdJBJqdpFIqNlFIqFmF4lEbfZnLwTwOIC2AByAyc65h8zsbgDXAtiZ+NLfOOfmsOuqqKiga3mHxj7ZGufbt2+ntaE5wk8++STN2Vh6aKz5rLPOovn+/ftpzuYoA3yufps2bWjtyy+/TPOf//znNB82bBjN2TkCN998M60Nfd9Tp06l+d///ndvxsaqAaBx48Y0nzRpEs1Hjx5NczafPbS3+6233urN2JoQtTmppgLALc655WbWFMAyM5uXyMY55+6vxXWISIbVZn/2bQC2JT7fb2ZrAPDTqkQk6/xXf7ObWUcAPQAsTlz0KzN738ymmlmNr2XNbJSZlZhZSWgLJhE5fmrd7GbWBMBMAKOdc58BmAigC4AzUfXM/0BNdc65yc65Iudc0YknnlgHhywiyahVs5tZDqoafbpz7nkAcM7tcM4dc85VApgC4Jzjd5gikqpgs5uZAfgbgDXOuQerXV59ydMhAFbV/eGJSF0JbtlsZn0BvAFgJYAv9jX+DYBhqHoJ7wCUArgu8WaeV0FBgbv++uu9edOmTemx5OTkeLPQ+wGlpaU0Dw2PsesPTUncsmULzUNTXHft2kVztmVzaDgzdNuhZbA7d+5M89zcXG92xhln0Fo2PRYIP2bsMW/dujWt3bRpE83ZkugAsGjRIpp36NDBm4WmHbNhwfvuuw8ff/xxjVs21+bd+EUAaiqmY+oikl10Bp1IJNTsIpFQs4tEQs0uEgk1u0gk1OwikUjrUtJmhhNO8P9+YdNfAeDIkSNJ33a/fv1o/sADNZ7t+x833HCDN2vRogWtDX1foe2DQ+P47733njcLTdVk00ABYOjQoTQfPnw4zW+88UZvNn78eFo7YMAAmi9btozm7PTs0HbRV155Jc1D4+g9e/akee/evb1ZWVkZrX3rrbe8WUVFhTfTM7tIJNTsIpFQs4tEQs0uEgk1u0gk1OwikVCzi0QiOJ+9Tm/MbCeA6hOFWwHgk7UzJ1uPLVuPC9CxJasuj62Dc67Gyfppbfav3bhZiXOuKGMHQGTrsWXrcQE6tmSl69j0Ml4kEmp2kUhkutknZ/j2mWw9tmw9LkDHlqy0HFtG/2YXkfTJ9DO7iKSJml0kEhlpdjMbaGbrzGy9mf06E8fgY2alZrbSzFaYWUmGj2WqmZWZ2apql+WZ2Twz+yjxke8Xnd5ju9vMtibuuxVmdkmGjq3QzF4zsw/MbLWZ3Zi4PKP3HTmutNxvaf+b3czqAfgQwEUAtgBYCmCYc+6DtB6Ih5mVAihyzmX8BAwz6wfgAIDHnXOnJS67F8Ae59w9iV+ULZ1zt2fJsd0N4ECmt/FO7FaUX32bcQCDAVyFDN535LguRxrut0w8s58DYL1zbqNz7giAGQAGZeA4sp5zbiGAPV+5eBCAxxKfP4aqH5a08xxbVnDObXPOLU98vh/AF9uMZ/S+I8eVFplo9gIAm6v9fwuya793B2CumS0zs1GZPpgatK22zdZ2AG0zeTA1CG7jnU5f2WY8a+67ZLY/T5XeoPu6vs65ngC+D+D6xMvVrOSq/gbLprHTWm3jnS41bDP+H5m875Ld/jxVmWj2rQCq74rXPnFZVnDObU18LAMwC9m3FfWOL3bQTXzkqxOmUTZt413TNuPIgvsuk9ufZ6LZlwLoamadzKwBgKEAZmfgOL7GzHITb5zAzHIBXIzs24p6NoARic9HAHghg8fyJdmyjbdvm3Fk+L7L+Pbnzrm0/wNwCarekd8A4M5MHIPnuDoDeC/xb3Wmjw3A06h6WXcUVe9tXAPgJADzAXwE4FUAeVl0bE+gamvv91HVWPkZOra+qHqJ/j6AFYl/l2T6viPHlZb7TafLikRCb9CJRELNLhIJNbtIJNTsIpFQs4tEQs0uEgk1u0gk/gf1aW/x+LIbGAAAAABJRU5ErkJggg==)     \n",
    "\n",
    "le GAN apprendra à générer des images de chiffres comme celui-ci (dans cette image, ce chiffre est '1')\n",
    "\n",
    "![ExampleDigit.png](data:image/png;base64,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)\n",
    "\n",
    "Il existe de nombreux types de GAN, chacun avec des avantages et des inconvénients différents. Dans ce notebook, nous travaillerons avec un DCGAN (Deep Convolutional GAN). Un DCGAN est similaire à un GAN, mais introduit des couches convolutives dans son réseau pour augmenter les performances sur le réseau simple et entièrement connecté du GAN. Les couches convolutives sont couramment utilisées dans les réseaux de neurones convolutifs, ainsi le DCGAN montrera comment celles-ci peuvent être traduites dans la conception d'un générateur et d'un discriminateur de GAN. Pour passer en revue ces concepts, vous pouvez regarder la vidéo facultative dans le module 6 sur Brightspace pour en savoir plus sur les couches convolutives dans les réseaux de neurones convolutifs (CNN).\n",
    "\n",
    "Comme évoqué ci-dessus, les DLA tels que les GAN nécessitent un immense traitement de calcul. Cela nécessite généralement une carte graphique haut de gamme, comme les dernières cartes de NVIDIA (qui fournissent des outils, tels que [CUDA](https://developer.nvidia.com/cuda-zone) pour permettre l'exécution du code sur le GPU pour un traitement rapide). Étant donné que nous ne pouvons pas nous attendre à ce que vous ayez une carte graphique ou que vous passiez le temps à attendre qu'un processeur effectue les calculs, nous utiliserons un environnement Jupyter gratuit basé sur le cloud fourni par Google. [Google Colab](https://colab.research.google.com/) est un environnement jupyter gratuit basé sur le cloud, idéal pour effectuer des expériences d'apprentissage en profondeur de base, car il permet d'accéder à des cartes graphiques très puissantes (qui ont limites, mais celles-ci ne devraient pas se produire pendant la durée de ce notebook). Ces limites incluent les contraintes de temps et les limitations de mémoire qui peuvent se produire si nous exécutons un algorithme d'entraînement pendant trop d'époques ou si nous concevons un modèle qui a trop de nœuds cachés. Ainsi, vous utiliserez cet environnement pour ce notebook et vous devrez suivre attentivement les instructions pour vous assurer que vous travaillez correctement dans le nouvel environnement. Contrairement aux notebooks précédents, ce notebook se concentrera sur l'exploration d'idées et l'analyse des résultats car il serait au-delà de la portée du cours d'apprendre une nouvelle bibliothèque complexe et de programmer un modèle complexe d'apprentissage profond avec elle.\n",
    "\n",
    "Ce notebook est basé sur [l'exemple officiel de TensorFlow de création d'un DCGAN](https://www.tensorflow.org/tutorials/generative/dcgan). Le code utilisé ici est inspiré de l'exemple et adapté pour optimiser les performances et fonctionner dans Google Colab. Ainsi, notez que le code utilisé ici provient de cet exemple avec des modifications si nécessaire.\n",
    "\n",
    "**Avant de démarrer ce notebook, créez un dossier à la racine de votre Google Drive nommé *CSI4106_Notebook9* (le chemin complet du fichier après l'ajout de ce dossier est /content/drive/My Drive/CSI4106_Notebook9). Ce répertoire sera utilisé plus tard dans le notebook pour enregistrer les modèles entraînés.**\n",
    "\n",
    "**Lors de la soumission de ce notebook, assurez-vous de NE PAS réinitialiser les sorties de l'exécution du code (et n'oubliez pas de sauvegarder le notebook avec ctrl + s).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdkrlU777hBX"
   },
   "source": [
    "***DEVOIR***:\n",
    "\n",
    "Parcourez le notebook en exécutant chaque cellule, une à la fois.\n",
    "Recherchez **(TO DO)** pour les tâches que vous devez effectuer. Ne modifiez pas le code en dehors des questions auxquelles vous êtes invité à répondre à moins que cela ne vous soit spécifiquement demandé. Une fois que vous avez terminé, signez le notebook (à la fin du notebook) et soumettez-le.\n",
    "\n",
    "*Le notebook sera noté le 25.\n",
    "Chaque **(TO DO)** a un certain nombre de points qui lui sont associés.*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zwn2MjEV9fUc"
   },
   "source": [
    "**1.0 - Configuration de l'environnement Google Colab**\n",
    "\n",
    "Avant d'entrer directement dans le code, nous devons d'abord configurer notre environnement Google Colab. Pour ce faire, nous devrons installer certaines bibliothèques avec pip (nous devrons le faire chaque fois que nous exécutons le notebook), nous devrons importer certaines bibliothèques et nous devrons nous connecter à notre Google Drive personnel (via votre compte de l'université, qui fonctionne même après la migration vers Outlook), et nous devrons enfin activer l'accès GPU pour ce notebook dans Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ON0A993QGYx8"
   },
   "source": [
    "Nous allons d'abord initialiser l'accès GPU pour le notebook. Google vous demandera de l'éteindre lorsqu'il n'est pas utilisé, mais vous pourrez facilement compléter le notebook sans problème. Il existe certaines limitations sur la durée de fonctionnement consécutif (plusieurs heures) et la mémoire totale pouvant être utilisée (environ 12 Go selon la carte allouée), mais ce ne sera pas un problème pour ce notebook. De plus, Google peut attribuer différents GPU à chaque utilisateur. Cela peut signifier que vous obtenez une carte légèrement plus lente ou plus rapide, mais tous les GPU seront plus que suffisants pour exécuter ces modèles avancés avec facilité. Si un problème lié au GPU se produit, vous devrez redémarrer le notebook ou vous déconnecter/vous reconnecter à un nouveau GPU après quelques minutes. Cependant, il est peu probable que cela se produise pendant le portable\n",
    "\n",
    "**Pour vous connecter à un GPU, allez dans *Modifier(Edit)* dans la barre d'outils, sélectionnez *Paramètres du notebook(Notebook Settings)*, sélectionnez *GPU* comme accélérateur matériel, puis cliquez sur *Enregistrer*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIzX2_rlQ5nI"
   },
   "source": [
    "Ensuite, exécutez les installations pip suivantes et les fonctions d'importation pour configurer TensorFlow et toutes les autres bibliothèques que nous utiliserons. Notez que plusieurs bibliothèques que nous avons utilisées dans les notebooks précédents reviennent pour résoudre ce problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sUKqfFLDcWJ"
   },
   "outputs": [],
   "source": [
    "# Install the specified libraries\n",
    "!pip install -q imageio\n",
    "!pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkXguTw3-c6O"
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow (note that this also will provide us with the MNIST dataset thanks to Keras)\n",
    "import tensorflow as tf\n",
    "import tensorflow_docs.vis.embed as embed\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Install additional libraries to help define our arrays, images, and more\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "# This library allows us to connect to our Google Drive from Google Colab to read and write files to/from\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCAZ3def_B3Q"
   },
   "source": [
    "Ensuite, vous devrez monter votre Google Drive comme répertoire utilisé par Google Colab. Nous ne pouvons pas utiliser le stockage local, tous les fichiers utilisés devront donc se trouver dans votre Google Drive. Il s'agit de l'emplacement de fichier par défaut dans le notebook, nous autorisons donc l'accès à votre Google Drive dans le code ci-dessous. Lors de l'exécution du code ci-dessous, il vous sera demandé d'autoriser l'accès à votre Drive (disque). Même lors de la soumission du notebook, aucune de vos informations ne sera enregistrée sur le notebook (nous ne pourrons pas accéder à votre lecteur), veillez donc à insérer l'autorisation d'accès au lecteur afin de continuer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ae0MbkQ5-5Gr"
   },
   "outputs": [],
   "source": [
    "# Mount google drive containing the datasets\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf49-SoMAI2U"
   },
   "source": [
    "Maintenant que nous avons monté Google Drive, nous pouvons accéder aux répertoires dans le Drive. Par exemple, puisque nous avons le répertoire *CSI4106_Notebook9* à la racine de *My Drive*, nous pouvons accéder à ce chemin de fichier via le répertoire suivant:\n",
    "- /content/drive/My Drive/CSI4106_Notebook9 (Ne cliquez pas, c'est juste pour afficher le chemin du fichier en fonction du dossier que vous avez créé)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiS37iPTJSTJ"
   },
   "source": [
    "**2.0 - Importation de l'ensemble de données**\n",
    "\n",
    "Maintenant que l'environnement est prêt, nous allons charger l'ensemble de données MNIST qui nous est fourni par les ensembles de données standard dans TensorFlow. L'ensemble de données MNIST est un ensemble de données composé d'images de chiffres manuscrits (1, 2, ..., 9). Ci-dessous, nous chargeons les données qui nous serviront d'ensemble d'apprentissage (rappelons que nous n'avons pas d'ensemble de test puisque le GAN s'entraînera continuellement pour s'améliorer, contrairement à la classification des chiffres qui nécessiterait un ensemble de test). Ensuite, nous examinons le nombre d'images que nous utiliserons pour nous entraîner dans l'ensemble de données et afficherons une image d'exemple avec le chiffre qu'elle représente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fetGkFPMKD_-"
   },
   "outputs": [],
   "source": [
    "# Load the handwritten digit images and their labels from the dataset\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Zx5uPPqvVmX"
   },
   "outputs": [],
   "source": [
    "# Show how many images are in the dataset\n",
    "print(\"There are\", len(train_images), \"many handwritten digits that will be used for training the DCGAN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XlP5DFDvyyB"
   },
   "outputs": [],
   "source": [
    "# Which digit is being shown\n",
    "print(\"The following image represents the handwritten digit\", train_labels[0])\n",
    "# What are its dimensions\n",
    "print(\"The image has the dimensions:\", train_images[0].shape, \"and is grayscale.\")\n",
    "# Display the digit\n",
    "plt.imshow(train_images[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZZoZUOi0htH"
   },
   "source": [
    "Ci-dessous, nous pouvons voir les valeurs réelles de l'image en gris. Comme vous pouvez le voir, chaque pixel reçoit une valeur unique entre 0 et 255 pour représenter l'échelle de gris à afficher pour ce pixel. Nous regardons cela car nous normaliserons ces valeurs de pixel et modifierons la structure de l'image pour qu'elle soit prête à être utilisée lors de l'entraînement par batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54hQy9cL08a2"
   },
   "outputs": [],
   "source": [
    "# Understand the structure of the image and the pixel values used.\n",
    "train_images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltL0M95VKWO_"
   },
   "source": [
    "Ensuite, nous modifierons les données reçues pour qu'elles soient prêtes à être utilisées par les algorithmes d'apprentissage. Nous modifions la structure pour ajouter une dimension supplémentaire qui indique simplement que l'ensemble de pixels représente un seul chiffre. Cela aide car nos modèles devront travailler avec des lots d'images et savoir clairement quels pixels appartiennent à quelle image.\n",
    "\n",
    "Plus les dimensions d'une image sont petites moins nous avons de valeurs par pixel (c'est-à-dire 1 pour les niveaux de gris ou noir et blanc et 3 pour RGB) ce qui emmène à un entraînement plus rapide (mais une perte de données si la taille est réduite). La taille de cette image est correcte (28 par 28), mais nous normaliserons les valeurs de pixel de 0 à 255 pour être comprises entre -1 et 1. Puisque les algorithmes d'apprentissage automatique apprennent à partir de données, nous voulons généralement normaliser des valeurs plus grandes à partir de valeurs plus petites pour garantir que le modèle apprenne les modèles corrects et minimise le coût des calculs avec de grands nombres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unO-QyN5KMZh"
   },
   "outputs": [],
   "source": [
    "# Restructure the numpy array to display the pixel values as 28 (width) by 28 (height) by 1 (one image)\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmhT_JhF2GXK"
   },
   "source": [
    "Ci-dessous, nous examinons rapidement les valeurs de pixels de la première image de notre ensemble d'entraînement modifié (pour afficher la normalisation et la nouvelle structure) ainsi qu'une vue rapide de l'image elle-même (qui aura exactement la même apparence, mais sera tracée différemment pour correspondre à sa nouvelle structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Afa_9yi32gpU"
   },
   "outputs": [],
   "source": [
    "# Which digit is being shown\n",
    "print(\"The following image represents the handwritten digit\", train_labels[0])\n",
    "# Look at the updated image (will look the same)\n",
    "plt.imshow(train_images[0][:, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4bd2fz02YcN"
   },
   "outputs": [],
   "source": [
    "# Look at the normalized pixel values\n",
    "print(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wkjNbTQOosv"
   },
   "source": [
    "Avec les données d'entraînement correctement traitées, nous allons maintenant définir la taille du lot à utiliser (combien d'échantillons seront entraînés à la fois) et nous définirons l'ensemble d'apprentissage complet à partir des ensembles de lots d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GhglZQcOnTG"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000 # Define the amount to shuffle the images\n",
    "BATCH_SIZE = 256 # Define the batch size\n",
    "# Batch and shuffle the data with a random seed of 0\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE, seed=0).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycb-4MQnRMO1"
   },
   "source": [
    "**3.0 - Définition et entrainement du DCGAN**\n",
    "\n",
    "Avec l'ensemble de données d'images chargé en lots (batchs), nous allons maintenant définir notre DCGAN. Pour ce faire, nous définirons les différentes couches au sein des réseaux de neurones qui sont utilisées par le générateur et le discriminateur. Chacun de ces modèles nécessitera son propre réseau de neurones, car les deux sont en concurrence.\n",
    "\n",
    "Ci-dessous, nous définissons le générateur comme un ensemble séquentiel de couches convolutives et de fonctions d'activation (*Leaky ReLU est utilisé ici*) qui prend une entrée et produit une image 28 par 28 par 1. Les valeurs transmises au modèle sont des valeurs aléatoires (appelées *bruit*) basées sur une distribution de probabilité. Le nombre total de valeurs utilisées comme entrée est appelé la *dimension latente* (dans cet exemple, il est de 40). Le but du générateur est de faire passer le bruit à travers son réseau neuronal pour aboutir à un chiffre manuscrit en sortie.\n",
    "\n",
    "Le générateur défini ci-dessous a quatre couches au total. La couche d'entrée accepte le bruit aléatoire comme entrée (produisant un plus grand nombre de sorties), effectue la *normalisation par lots (batchs)* pour aider à améliorer les résultats et exécute la sortie via la fonction d'activation *Leaky ReLU*. Ceci entre ensuite dans la première des trois couches convolutives qui s'alimentent chacune après avoir effectué la *normalisation par lots* et l'utilisation de la fonction d'activation *Leaky ReLU*. Celles-ci réduisent chacune le nombre de sorties à la taille finale de l'image que l'on souhaite (28 par 28 par 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ri4DZOt4PXwO"
   },
   "outputs": [],
   "source": [
    "def make_generator_model(batch_size):\n",
    "    '''\n",
    "    Defines a Generator to accept rnadom noise based on a probability distribution\n",
    "    and run it through a Neural Network of three convolutional layers with the Leaky ReLU\n",
    "    activiation function and Batch Normalization (which makes the learning process faster and more stable)\n",
    "    '''\n",
    "    model = tf.keras.Sequential()\n",
    "    # Layer 1\n",
    "    model.add(layers.Dense(7*7*batch_size, use_bias=False, input_shape=(40,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((7, 7, batch_size)))\n",
    "    assert model.output_shape == (None, 7, 7, batch_size) # Note: None is the batch size\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Layer 4\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    # Return the Generator\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LijKVM7bHJtu"
   },
   "source": [
    "Ci-dessous, nous créons le générateur que nous utiliserons et examinons un exemple de sortie lors du passage d'un bruit aléatoire dans le modèle non entraîné. Comme vous pouvez le voir, il n'a actuellement aucune représentation d'un chiffre manuscrit car il n'a pas appris les modèles à partir des images de chiffres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-D9r9fscPpr-"
   },
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "generator = make_generator_model(BATCH_SIZE)\n",
    "# Generate a random noise input\n",
    "noise = tf.random.normal([1, 40])\n",
    "# Retrieve the outputted image from the Generator for the input\n",
    "generated_image = generator(noise, training=False)\n",
    "# Display the image generated by the Generator\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PP63JKGlJjuP"
   },
   "source": [
    "Ensuite, nous définissons le discriminateur. Le discriminateur accepte une image comme entrée (avec les dimensions attendues) et l'exécute à travers un simple ensemble de couches convolutives pour afficher si l'image est réelle ou fausse. Ce modèle s'entraînera contre le générateur dans le but d'apprendre les attributs des chiffres manuscrits faux et réels. Le discriminateur est défini comme ayant deux couches convolutives qui sont essentiellement les mêmes que celles que nous avons utilisées dans le générateur. Notons que les couches convolutionnelles augmentent en taille, plutôt que de diminuer en taille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_88c3ZzPq8G"
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    '''\n",
    "    Create a Discriminator with two convolutional layers that accept a\n",
    "    handwritten digit image as input and outputs whether it is true or false.\n",
    "    Note that the Dropout code refers to the Dropout regularization technique that \n",
    "    can be used to enhance the performance of a Neural Network and achieve strong\n",
    "    results quicker than normal.\n",
    "    '''\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXH0E3mMKM60"
   },
   "source": [
    "Ci-dessous, nous montrons ce que le discriminateur non entraîné produit lorsque nous fournissons l'image créée par le générateur dans l'exemple ci-dessus comme entrée. Notez que les nombres négatifs signifient que l'image est prédite comme étant fausse tandis que les nombres positifs signifient que l'image est prédite comme étant réelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Sw7zLZePsWP"
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A_3bfIWP837"
   },
   "source": [
    "Une fois les modèles définis, nous allons maintenant définir les fonctions de perte à utiliser lors de l'entraînement et les optimiseurs à utiliser. Nous n'entrerons pas dans les détails concernant le code ici, sauf pour le paramètre utilisé pour l'optimiseur Adam. Cependant, des commentaires ont été ajoutés pour expliquer le code à un niveau élevé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGRwZ6HnPx9f"
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYWWrFtTP08W"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    '''\n",
    "    The loss function for the discriminator.\n",
    "    This must consider the combined loss from how well it performs at detecting fake and\n",
    "    real images.\n",
    "    '''\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qi171flMP3Z_"
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    '''\n",
    "    The Generator loss is simply based on whether the generated image was able to \n",
    "    trick the discriminator in believing that the fake image is real.\n",
    "    '''\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWGgbZ02NRjQ"
   },
   "source": [
    "Ci-dessous, les optimiseurs utilisés pour entraîner les modèles sont sélectionnés. Nous avons également utilisé des optimiseurs lorsque nous travaillons dans le notebook MLP avec scikit-learn. La principale chose à noter ici est que le nombre passé en entrée est le taux d'apprentissage à utiliser par les modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLncRgryP4Ve"
   },
   "outputs": [],
   "source": [
    "# Define the learning rate to be used by the optimizers\n",
    "lr = 1e-4\n",
    "# Set the optimizers that will be used by both models and set the learning rate.\n",
    "generator_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVlS9HsrOPv7"
   },
   "source": [
    "Ensuite, cette étape facultative est fournie par le didacticiel TensorFlow pour montrer comment nous pouvons enregistrer un modèle à différents états pendant le processus d'entrainement. Comme nous entraînons les modèles à travers de nombreuses époques, cela nous permettra de sauvegarder l'état d'un modèle et de le charger à tout moment. Notez que ce qui suit suppose que vous avez défini le répertoire de votre lecteur comme détaillé dans l'introduction de ce notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KM4j072ibATM"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = '/content/drive/My Drive/CSI4106_Notebook9'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beIKnJQ-OyPj"
   },
   "source": [
    "De la même manière que pour le notebook MLP, nous déterminons maintenant pour combien d'époques le modèle doit être entraîné. Bien que cela prenne plusieurs minutes, 50 suffiront pour voir comment les fausses images du modèle évoluent. Les autres paramètres sont la *dimension latente* qui sera utilisée et le nombre d'exemples que nous verrons pour chaque époque (pour voir comment le générateur s'améliore au fil du temps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2kjCYcWP6hm"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 40\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim], seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxYwy7TZP2H-"
   },
   "source": [
    "Nous allons maintenant définir les fonctions d'entrainement. La fonction *train_step* ci-dessous accepte un lot d'images de l'ensemble d'entraînement comme entrée et a d'abord collecté un lot de fausses images à partir du générateur. Le Dirscriminator tente alors de déterminer, à partir de l'ensemble des images réelles et de l'ensemble des fausses images, quelles images sont réelles et lesquelles sont fausses. La perte des deux modèles est ensuite calculée pour trouver les gradients et pour faire la rétropropagation à travers les modèles pour mettre à jour le générateur et le discriminateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TTrNxuRQKQu"
   },
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images, batch_size):\n",
    "    '''\n",
    "    For a single batch of images from the training set, train the Discriminator\n",
    "    and Generator.\n",
    "    This function will automatically run on the default GPU of the system if it\n",
    "    can be detected by the environment.\n",
    "    '''\n",
    "    # Generate the batch of random noise inputs to be used to create the fake images\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      # Generate the fake images\n",
    "      generated_images = generator(noise, training=True)\n",
    "      # Have the Discriminator determine which images are real or fake from both sets of images\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "      # Calculate the loss for both the generator and the discriminator\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # Compute the gradients for both the Generator and the Dsicriminator\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    # Update the models by applying the gradients to the models\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNMj1xb8SAs1"
   },
   "source": [
    "De manière similaire à ce que nous avons utilisé dans le notebook MLP, nous créons une fonction d'apprentissage qui entraîne chaque modèle avec l'intégralité de l'ensemble d'apprentissage (par lots) pendant un certain nombre d'époques. Après chaque époque, un ensemble de 16 exemples de fausses images sont générés pour visualiser le processus. Les points de contrôle sont également créés dans le dossier Drive spécifié pour une utilisation ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2eE_VSRQQAe"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    '''\n",
    "    Trains a Generator and Discriminator with a specified dataset for a specified\n",
    "    number of Epochs. Example outputs are saved to be visualized later.\n",
    "    '''\n",
    "    for epoch in range(epochs):\n",
    "      start = time.time()\n",
    "      # Loop through each batch in the training set\n",
    "      for image_batch in dataset:\n",
    "        # Train the models with the specified batch\n",
    "        train_step(image_batch, BATCH_SIZE)\n",
    "\n",
    "      # Produce images for the GIF as we go\n",
    "      display.clear_output(wait=True)\n",
    "      generate_and_save_images(generator,\n",
    "                              epoch + 1,\n",
    "                              seed)\n",
    "\n",
    "      # Save the model every 15 epochs\n",
    "      if (epoch + 1) % 15 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "      print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                            epochs,\n",
    "                            seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FInn6iZFQR1t"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  '''\n",
    "  Generates fake images based on the Generator after being trained for a \n",
    "  specified number of epochs. We set the model to not train while generating the\n",
    "  images to ensure that only the training function will train the model.\n",
    "  '''\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KilxQSGwTtBd"
   },
   "source": [
    "Maintenant, nous allons entrainer le DCGAN! Chaque époque prendra environ 10 à 12 secondes, vous verrez donc comment elle se met à jour au cours des 50 époques pour lesquelles nous allons l'exécuter. En réalité, nous pouvons optimiser davantage le modèle ou fonctionner pendant plus d'époques, mais cela suffit pour visualiser la progression dans le notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmX2U3O2QZHF"
   },
   "outputs": [],
   "source": [
    "# Remember to turn ON the GPU in Google Colab (the speed that it runs at will depend on the GPU that you are assigned;\n",
    "#                                              between 10 and 30 seconds on average)!!!\n",
    "# You are expected to run this through all of the epochs, not just a subset of them since everyone has the computation power for it.\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwe92a6hc1rr"
   },
   "source": [
    "Une fois l'entrainement terminé, nous chargerons le point de contrôle de notre générateur pour analyser les résultats de l'entrainement. Après avoir chargé le point de contrôle, nous afficherons une partie de la collection de fausses images que nous avons affichées pour l'époque 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuiWu6smQaM1"
   },
   "outputs": [],
   "source": [
    "# Load the checkpoint that is stored in your Google Drive folder\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-BxSYmLcHVE"
   },
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Olh_SRCcJeD"
   },
   "outputs": [],
   "source": [
    "# Display the output from the last epoch (epoch 50)\n",
    "plt.imshow(display_image(EPOCHS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCBNxQvxgkmv"
   },
   "source": [
    "**(TO DO) Q1**\n",
    "\n",
    "a) Pour vous aider à analyser l'évolution des images générées pendant l'apprentissage du générateur, définissez la fonction *display_key_epochs* ci-dessous. Cette fonction doit afficher chaque collection de fausses images qui ont été produites pour les époques *[10, 20, 30, 40, 50]*. Une fois la fonction définie, appelez-la pour afficher chaque collection de fausses images des époques spécifiées. Le code ci-dessus de cette question montre comment vous pouvez récupérer les fausses sorties pour une époque spécifique.\n",
    "\n",
    "b) D'après les résultats de l'observateur de (a), le modèle semble-t-il bien fonctionner (c'est-à-dire que les chiffres générés ressemblent-ils à des chiffres manuscrits)?\n",
    "\n",
    "c) Après avoir visualisé les fausses images à travers les époques, qu'arrive-t-il aux sorties générées alors qu'elles continuent à être entraînées à chaque époque? Expliquez pourquoi ce processus se produit.\n",
    "\n",
    "d) Au cours des dernières époques, à quel point les changements apportés aux fausses images sont-ils drastiques? Semblent-ils s'améliorer constamment au même rythme vers la fin de l'entrainement? Pourquoi ou pourquoi pas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEzNGBKJYNq8"
   },
   "source": [
    "**(TO DO) Q1 (a) - 2 points**    \n",
    "a) Pour vous aider à analyser l'évolution des images générées pendant l'apprentissage du générateur, définissez la fonction *display_key_epochs* ci-dessous. Cette fonction doit afficher chaque collection de fausses images qui ont été produites pour les époques *[10, 20, 30, 40, 50]*. Une fois la fonction définie, appelez-la pour afficher chaque collection de fausses images des époques spécifiées. Le code ci-dessus de cette question montre comment vous pouvez récupérer les fausses sorties pour une époque spécifique.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_U2oDRnu7Ci5"
   },
   "outputs": [],
   "source": [
    "# TODO: Define the function for the target epochs and use it to display the images.\n",
    "# Note: This function is used later in the notebook and assumes that no input parameters will be added.\n",
    "def display_key_epochs():\n",
    "  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyMj6i3KbRnm"
   },
   "source": [
    "**(TO DO) Q1 (b) - 1 point**   \n",
    "b) D'après les résultats de l'observateur de (a), le modèle semble-t-il bien fonctionner (c'est-à-dire que les chiffres générés ressemblent-ils à des chiffres manuscrits)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-B92DSLIbSrE"
   },
   "source": [
    "TODO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOZTM9HtbTC4"
   },
   "source": [
    "**(TO DO) Q1 (c) - 2 points**   \n",
    "\n",
    "c) Après avoir visualisé les fausses images à travers les époques, qu'arrive-t-il aux sorties générées alors qu'elles continuent à être entraînées à chaque époque? Expliquez pourquoi ce processus se produit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmunYyTfbSH-"
   },
   "source": [
    "TODO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e-dfpB5bR-W"
   },
   "source": [
    "**(TO DO) Q1 (d) - 2 points**   \n",
    "d) Au cours des dernières époques, à quel point les changements apportés aux fausses images sont-ils drastiques? Semblent-ils s'améliorer constamment au même rythme vers la fin de l'entrainement? Pourquoi ou pourquoi pas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ueSO1QBbRP8"
   },
   "source": [
    "TODO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLYSlV-akhL9"
   },
   "source": [
    "**4.0 - Test de différents hyperparamètres**\n",
    "\n",
    "Après avoir vu comment l'ensemble du processus est effectué, vous allez essayer le même processus pour vous-même, mais en utilisant différents hyperparamètres. En apportant des modifications mineures à un petit nombre de paramètres, il est possible d'obtenir des résultats très différents. Après avoir terminé ce processus, vous discuterez de la façon dont les résultats changent par rapport au test qui vous est fourni dans l'exemple ci-dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYzn2vU5lL9G"
   },
   "source": [
    "**(TO DO) Q2 - 6 points**    \n",
    "Vous allez maintenant créer un nouveau générateur et discriminateur de telle sorte qu'ils fonctionneront avec une *taille de lot* de 64 images par lot et définiront l'optimiseur pour utiliser un *taux d'apprentissage* plus élevé de 1e-3. Cela ne nécessitera aucune mise à jour des structures définies du générateur et du discriminateur et impliquera simplement de copier du code et d'apporter des modifications mineures. Vous trouverez ci-dessous une liste de chaque tâche que vous devrez effectuer. Chacun de ces éléments doit être effectué dans la cellule de code correspondante ci-dessous. La structure est fournie pour vous, vous devez donc simplement remplir les espaces vides.\n",
    "\n",
    "1) Mettez à jour la variable de taille de lot pour spécifier que les lots doivent être constitués de 64 images et redéfinissez *train_dataset* pour utiliser la nouvelle taille de lot.\n",
    "2) Redéfinissez l'objet Generator avec la nouvelle taille de lot comme entrée de la fonction.\n",
    "3) Redéfinissez l'objet Discriminator.\n",
    "4) Redéfinissez la variable *cross_entropy* de la même manière qu'auparavant.\n",
    "5) Réglez le taux d'apprentissage sur 1e-3 et redéfinissez les variables de l'optimiseur.\n",
    "6) Exécutez la fonction *train_step* fournie pour recompiler le code après avoir effectué les modifications ci-dessus.\n",
    "7) Entrainer les nouveaux modèles pour 50 époques avec l'ensemble de données défini en 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBwigInSdkVZ"
   },
   "outputs": [],
   "source": [
    "# TODO: 1) Update the batch size variable to specify that the batches should consist of 64 images and redefine train_dataset to use the new batch size. \n",
    "# Update the batch size\n",
    "BATCH_SIZE = ...\n",
    "# Update train_dataset\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9ELACgoqmru"
   },
   "outputs": [],
   "source": [
    "# TODO: 2) Redefine the Generator object with the new batch size as input to the function. \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0wPjNKpqpU-"
   },
   "outputs": [],
   "source": [
    "# TODO: 3) Redefine the Discriminator object.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BL7ZReOprGbs"
   },
   "outputs": [],
   "source": [
    "# TODO: 4) Redefine the cross_entropy variable the same way it was previously\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MouOPyjq0qX"
   },
   "outputs": [],
   "source": [
    "# TODO: 5) Set the learning rate to 1e-3 and re-define the optimizer variables\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMniHxORyb1K"
   },
   "outputs": [],
   "source": [
    "# (6) Re-compile this function to work with the updates (just run this code cell)\n",
    "@tf.function\n",
    "def train_step(images, batch_size):\n",
    "    '''\n",
    "    For a single batch of images from the training set, train the Discriminator\n",
    "    and Generator.\n",
    "    This function will automatically run on the default GPU of the system if it\n",
    "    can be detected by the environment.\n",
    "    '''\n",
    "    # Generate the batch of random noise inputs to be used to create the fake images\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      # Generate the fake images\n",
    "      generated_images = generator(noise, training=True)\n",
    "      # Have the Discriminator determine which images are real or fake from both sets of images\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "      # Calculate the loss for both the generator and the discriminator\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # Compute the gradients for both the Generator and the Dsicriminator\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    # Update the models by applying the gradients to the models\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hi0Lfs_Arl5Y"
   },
   "outputs": [],
   "source": [
    "# TODO: 7) Train the new models for 50 epochs with the dataset defined in 1)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqT_1YKnzt2N"
   },
   "source": [
    "**(TO DO) Q3**   \n",
    "Maintenant que vous avez modifié les hyperparamètres et entrainé le nouveau DCGAN, vous expliquerez comment ce modèle se compare au modèle de la partie 3.0 de ce notebook et quelques observations basées sur les résultats.\n",
    "\n",
    "a) Chargez le dernier point de contrôle depuis votre dossier Google Drive (de la même manière que précédemment) et utilisez votre fonction *display_key_epochs* pour afficher les résultats des époques clés à utiliser pour le reste de la question.\n",
    "\n",
    "b) Entre le modèle que vous avez défini et formé et l'exemple de modèle de la partie 3.0, qui a mieux fonctionné et pourquoi pensez-vous qu'il a mieux fonctionné?\n",
    "\n",
    "c) Nommez un pour et un contre de l'augmentation du taux d'apprentissage (sur base de vos observations et/ou sur base de ce que vous savez que fait le taux d'apprentissage).\n",
    "\n",
    "d) Nommez un pour et un contre en diminuant la taille du lot (en fonction de vos observations et/ou en fonction de ce que vous savez qui se passe lorsque la taille du lot est réduite)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KR10X8CUaVpy"
   },
   "source": [
    "**(TO DO) Q3 (a) - 1 point**   \n",
    "a) Chargez le dernier point de contrôle depuis votre dossier Google Drive (de la même manière que précédemment) et utilisez votre fonction *display_key_epochs* pour afficher les résultats des époques clés à utiliser pour le reste de la question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4y9z_XSaWAL"
   },
   "outputs": [],
   "source": [
    "# TODO: Restore the latest checkpoint and display the key epochs\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4-cFq8eKuqk"
   },
   "source": [
    "**(TO DO) Q3 (b) - 1 point**   \n",
    "b) Entre le modèle que vous avez défini et formé et l'exemple de modèle de la partie 3.0, qui a mieux fonctionné et pourquoi pensez-vous qu'il a mieux fonctionné?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M4SLbIAKvaT"
   },
   "source": [
    "TO DO ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaQmjP3VKv5R"
   },
   "source": [
    "**(TO DO) Q3 (c) - 2 points**   \n",
    "\n",
    "c) Nommez un pour et un contre de l'augmentation du taux d'apprentissage (sur base de vos observations et/ou sur base de ce que vous savez que fait le taux d'apprentissage).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbAX7-GzKwBB"
   },
   "source": [
    "TO DO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APej4biXKwQB"
   },
   "source": [
    "**(TO DO) Q3 (d) - 2 points**   \n",
    "d) Nommez un pour et un contre en diminuant la taille du lot (en fonction de vos observations et/ou en fonction de ce que vous savez qui se passe lorsque la taille du lot est réduite)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzHyl9tiKwWL"
   },
   "source": [
    "TO DO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47uiDNuT0gOp"
   },
   "source": [
    "**5.0 - Test d'un DCGAN plus simple**\n",
    "\n",
    "Maintenant que vous avez exploré un DCGAN de base et que vous avez joué avec le taux d'apprentissage et les hyperparamètres comme la taille des lots, nous allons passer par un dernier test d'un DCGAN encore plus simple.\n",
    "\n",
    "Dans ce scénario, tout sera le même que votre configuration dans la section 4.0 de ce notebook, sauf que nous redéfinirons le DCGAN pour supprimer l'une des couches convolutives du générateur et du discriminateur. Cela se traduira par un modèle plus simpliste. En comparant les résultats obtenus ici aux résultats obtenus dans la section précédente, vous discuterez si une complexité accrue ou une complexité réduite aide le modèle à générer de meilleurs faux chiffres manuscrits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02Z90B-ONMEy"
   },
   "source": [
    "Le générateur ci-dessous est maintenant configuré pour être le même qu'avant, mais ne contient maintenant que trois couches, avec deux couches convolutives. Plus précisément, la plus grande couche convolutive a été supprimée, ce qui fait que la sortie de la couche d'entrée conduit à une couche convolutionnelle plus petite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhLrM2og2Bxa"
   },
   "outputs": [],
   "source": [
    "def make_generator_model(batch_size):\n",
    "    '''\n",
    "    Defines a Generator to accept rnadom noise based on a probability distribution\n",
    "    and run it through a Neural Network of two convolutional layers with the Leaky ReLU\n",
    "    activiation function and Batch Normalization (which makes the learning process faster and more stable)\n",
    "    '''\n",
    "    model = tf.keras.Sequential()\n",
    "    # Layer 1\n",
    "    model.add(layers.Dense(7*7*batch_size, use_bias=False, input_shape=(40,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((7, 7, batch_size)))\n",
    "    assert model.output_shape == (None, 7, 7, batch_size) # Note: None is the batch size\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    # Return the Generator\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3RPOs1F2Z4G"
   },
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "generator = make_generator_model(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQ8LN7t5NlDu"
   },
   "source": [
    "De même, le Discriminateur a supprimé l'une de ses deux couches convolutives. Il en résulte une seule couche convolutionnelle pour apprendre à distinguer quelles images sont réelles et lesquelles sont fausses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EteEkB0D2IHW"
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    '''\n",
    "    Create a Discriminator with one convolutional layer that accept a\n",
    "    handwritten digit image as input and outputs whether it is true or false.\n",
    "    Note that the Dropout code refers to the Dropout regularization technique that \n",
    "    can be used to enhance the performance of a Neural Network and achieve strong\n",
    "    results quicker than normal.\n",
    "    '''\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sA_Q-w432cVl"
   },
   "outputs": [],
   "source": [
    "# Define the Discriminator\n",
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mg9w8Zwb2hkx"
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Set the optimizers that will be used by both models and set the learning rate.\n",
    "generator_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLNl4MyP2sYT"
   },
   "outputs": [],
   "source": [
    "# Re-compile this function to work with the updates\n",
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images, batch_size):\n",
    "    '''\n",
    "    For a single batch of images from the training set, train the Discriminator\n",
    "    and Generator.\n",
    "    This function will automatically run on the default GPU of the system if it\n",
    "    can be detected by the environment.\n",
    "    '''\n",
    "    # Generate the batch of random noise inputs to be used to create the fake images\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      # Generate the fake images\n",
    "      generated_images = generator(noise, training=True)\n",
    "      # Have the Discriminator determine which images are real or fake from both sets of images\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "      # Calculate the loss for both the generator and the discriminator\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # Compute the gradients for both the Generator and the Dsicriminator\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    # Update the models by applying the gradients to the models\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHBceA6Y2u4b"
   },
   "outputs": [],
   "source": [
    "# Remember to turn ON the GPU in Google Colab.\n",
    "# You are expected to run this through all of the epochs, not just a subset of them since everyone has the computation power for it.\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Si8tMA-N16fJ"
   },
   "source": [
    "**(TO DO) Q4**  \n",
    "\n",
    "a) Chargez le dernier point de contrôle depuis votre dossier Google Drive (de la même manière que précédemment) et utilisez votre fonction *display_key_epochs* pour afficher les résultats des époques clés à utiliser pour le reste de la question.\n",
    "\n",
    "b) Comment la suppression d'une des couches convolutives du générateur et du discriminateur a-t-elle affecté l'entrainement?\n",
    "\n",
    "c) Comment la suppression d'une des couches convolutives du générateur et du discriminateur a-t-elle affecté les résultats par rapport au modèle de la partie 4.0? En outre, comment la suppression de cela a-t-elle affecté les résultats vus des extrants?\n",
    "\n",
    "d) Si les images que nous utilisons étaient de plus grande taille, aurait-il été préférable d'avoir un modèle complexe ou un modèle simple dans ce scénario? Pourquoi ou pourquoi pas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msn1rXz_bv2d"
   },
   "source": [
    "**(TO DO) Q4 (a) - 1 point**     \n",
    "a) Chargez le dernier point de contrôle depuis votre dossier Google Drive (de la même manière que précédemment) et utilisez votre fonction *display_key_epochs* pour afficher les résultats des époques clés à utiliser pour le reste de la question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYvVLq5TbwOU"
   },
   "outputs": [],
   "source": [
    "# TODO ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCigdxeRO6P5"
   },
   "source": [
    "**(TO DO) Q4 (b) - 1 point**    \n",
    "b) Comment la suppression d'une des couches convolutives du générateur et du discriminateur a-t-elle affecté l'entrainement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrryS1I5PO9k"
   },
   "source": [
    "TODO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPzN2uHhPPQb"
   },
   "source": [
    "**(TO DO) Q4 (c) - 2 points**    \n",
    "c) Comment la suppression d'une des couches convolutives du générateur et du discriminateur a-t-elle affecté les résultats par rapport au modèle de la partie 4.0? En outre, comment la suppression de cela a-t-elle affecté les résultats vus des extrants?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0-CbolVPPUY"
   },
   "source": [
    "TODO ...     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsJIauTEPPC4"
   },
   "source": [
    "**(TO DO) Q4 (d) - 2 points**    \n",
    "d) Si les images que nous utilisons étaient de plus grande taille, aurait-il été préférable d'avoir un modèle complexe ou un modèle simple dans ce scénario? Pourquoi ou pourquoi pas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmrsUQzXPOtl"
   },
   "source": [
    "TODO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QLJJ7CFhnq7"
   },
   "source": [
    "***SIGNATURE:***\n",
    "Mon nom est --------------------------.\n",
    "Mon numéro d'étudiant est -----------------.\n",
    "Je certifie être l'auteur de ce devoir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gk5bNGV2hoOR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CSI4506-GAN_Automne20.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
